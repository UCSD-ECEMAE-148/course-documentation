{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>Welcome to the course documentation page for ECE/MAE 148: Intro to Autonomous Vehicles at University of California, San Diego.</p>"},{"location":"#what-youll-find-here","title":"What you'll find here","text":"<p>This page will contain all of the necessary documentation for your 10-week class experience, including step-by-step documentation for every component of your individual and group deliverables, troubleshooting, final project documentation, and reference examples.</p>"},{"location":"#about-this-course","title":"About this course","text":"<p>ECE/MAE 148 is a highly popular technical elective course for Engineering students across both disciplines. In this class you will receive invaluable hands-on experience in project-based learning as you work in teams to design, build and program a 1:10 scale robot vehicle capable of performing different tasks while autonomously driving, from simulation to deployment on your real, physical car. This won't be your usual classroom experience in that a majority of your learning will come from getting your hands dirty as opposed to written assignment exercises. </p> <p>In 148, your success is contingent upon your willingness to put in the minimum time expected for a 4-unit class, and then some. For example, if your team consists of 4 members, that's 40 hours per week in combined team efforts to reach your objectives. In return, you will walk away with a set of real, crucial skills to set you apart from everyone else when it comes time for graduate school or career applications; in short, it's well worth it, and you'll have a lot of fun doing it.</p>"},{"location":"#what-kind-of-skills-do-you-mean","title":"What kind of skills do you mean?","text":""},{"location":"course-deliverables/group/","title":"Group Assignments","text":""},{"location":"course-deliverables/group/#physical-robot","title":"Physical Robot","text":"<p>As the course progresses your group will be required to present physical evidence of your robot's progress, including videos of your physical robot completing the given objectives.</p>"},{"location":"course-deliverables/group/#hardware","title":"Hardware","text":"<p>TODO</p>"},{"location":"course-deliverables/group/#donkeycar-computer-vision","title":"DonkeyCar: Computer Vision","text":"<p>TODO: link documentation for physical autonomous laps in DonkeyCar</p>"},{"location":"course-deliverables/group/#donkeycar-gps-navigation","title":"DonkeyCar: GPS Navigation","text":"<p>TODO: link documentation for GPS laps in Donkey</p>"},{"location":"course-deliverables/group/#ros2","title":"ROS2","text":""},{"location":"course-deliverables/group/#line-following-laps","title":"Line-Following Laps","text":"<p>TODO: line following docs</p>"},{"location":"course-deliverables/group/#lane-following-laps","title":"Lane-Following Laps","text":"<p>TODO: lane following docs</p>"},{"location":"course-deliverables/group/#final-project-progress-reports","title":"Final Project Progress Reports","text":"<p>Starting around Week 6 your group will give a short, 5-minute presentation to your classmates at the end of each week. These progress reports should be no more than 3-5 slides discussing what goals you have met, what you are still working on, what is working, and what still needs debugging.</p>"},{"location":"course-deliverables/individual/","title":"Individual Assignments","text":""},{"location":"course-deliverables/individual/#donkeysim","title":"DonkeySim","text":"<p>During this course, you will need to upload a video of your trained model car successfully completing 3 autonomous laps on the DonkeySim track using three different scenarios that are outlined in the Virtual Machine Guide.</p>"},{"location":"course-deliverables/individual/#3-autonomous-laps-on-localhost","title":"3 Autonomous Laps on <code>localhost</code>","text":"<p>TODO: Link to DonkeyCar documentation for local host</p>"},{"location":"course-deliverables/individual/#training-on-the-gpu-cluster","title":"Training on the GPU Cluster","text":"<p>TODO: Link to documentation for GPU cluster</p>"},{"location":"course-deliverables/individual/#3-autonomous-laps-on-remote-server-roboticistdev","title":"3 Autonomous Laps on remote server <code>roboticist.dev</code>","text":"<p>TODO: Link to documentation for remote server</p>"},{"location":"course-deliverables/individual/#the-construct","title":"The Construct","text":"<p>TODO: link online course details</p>"},{"location":"course-overview/outline/","title":"Introductions and Class Logistics","text":""},{"location":"course-overview/outline/#instruction-team","title":"Instruction Team","text":"<ul> <li>Jack Silberman - jacks@eng.ucsd.edu - PhD, Faculty MAE, ECE and HDSI</li> <li>TBD - email - TA/Tutor</li> </ul>"},{"location":"course-overview/outline/#safety","title":"Safety","text":"<ul> <li>Follow the return to learn, that simple<ul> <li>Lab access. Please observe UCSD\u2019s safety guidelines - Compliance with required trainings.</li> <li>Be safe, ask for help; don\u2019t be afraid to test things but be safe. You are in school to learn, you are supposed to have help.</li> </ul> </li> </ul>"},{"location":"course-overview/outline/#adapting-to-changes","title":"Adapting to Changes","text":"<ul> <li>For the past 5+ years, more than 1/2 of you would not be in this class. Be patient with us.<ul> <li>We expanded it from 28 to 60 students, 7 to 15 Teams.</li> <li>This means we had to find a space to fit all of you in a lab environment. Please help by not talking at the same time as the instructional team.</li> </ul> </li> <li>We basically have double of the workload with the same resources.<ul> <li>Do your part and be patient when things don\u2019t work. Do some hacking and look for solutions with us. Don\u2019t expect solutions given to you without doing some work too.</li> </ul> </li> <li>We are using cutting-edge technology; it may cut us a bit while we master it.</li> </ul>"},{"location":"course-overview/outline/#logistics","title":"Logistics","text":"<ul> <li>Lectures and times are mixed since we do hands-on lab within the lectures.<ul> <li>Tuesdays and Thursdays: 5:30 - 6:50 PM<ul> <li>Office hours at the lab: 7:00 - 8:30 PM</li> </ul> </li> <li>Additional office hours TBD as needed</li> </ul> </li> <li>If needed, access to EBUII 339 TritonAI lab TBD with TA</li> <li>Discord for communication, Professor Silberman will share the invite link</li> <li>Assignments will be posted on Discord channels<ul> <li>Video evidence of course deliverables to be shared on Discord</li> </ul> </li> <li>See Grading Formula</li> <li>Top requests from past quarters<ul> <li>More time for final project</li> <li>Help on Python coding</li> <li>More structure on \"what\" and \"when\"</li> </ul> </li> <li>This is a 4 credit class. If you cannot dedicate 10 hours per week, please consider giving a chance to the dozens of other students on the waiting list. If you put in the effort, you will be successful.</li> <li>This class is hands-on-fun-busy where you will gain highly useful skills to add to your resume.</li> <li>There are no exams or lists of exercises to work on.<ul> <li>Your time will be used learning new skills that can make you stand out on job or graduate school applications.</li> <li>There are two 360 evaluations where your teammates will judge you. Also, other teams can help on your grade. More on that in grading formula.</li> </ul> </li> <li>CBT / e-book license on Robot Ignite Academy (The Construct) is required. You have homework already.</li> </ul>"},{"location":"course-overview/outline/#curriculum","title":"Curriculum","text":"<ul> <li>Deep Learning AI - Human Behave Cloning<ul> <li>Simulator on a virtual machine on your host computer.</li> <li>AI model Training using UCSD\u2019s Supercomputer Center.</li> <li>Multi-robots race online against racers from around the world.</li> <li>Autonomous laps using a Physical robot at UCSD\u2019s scale race track.</li> <li>Optional - We were invited for an event sponsored by a Bank in the Bay Area<ul> <li>Date to be confirmed - end of the quarter</li> <li>10 to 15 students sponsored to travel for a day in Oakland California</li> <li>Help companies founder train their robots with their kids then race; possible network opportunity for you.</li> </ul> </li> </ul> </li> </ul>"},{"location":"course-overview/syllabus/","title":"10-Week Course Syllabus","text":""},{"location":"course-overview/syllabus/#week-1","title":"Week 1","text":""},{"location":"course-overview/syllabus/#lecture-and-lab","title":"Lecture and Lab","text":"<ul> <li>Class expectations</li> <li>\"What will a robotics class enable me to do?\"<ul> <li>Search web for robotics jobs, i.e. keywords 'deep learning,' 'ROS2,' 'AI'</li> </ul> </li> <li>Grading formula</li> <li>Syllabus<ul> <li>Classes schedule and high level deliverables</li> </ul> </li> <li>Introduction to the class resources<ul> <li>Laboratory and tool</li> </ul> </li> <li>Laboratory and safety training</li> <li>Design robot, start building robot</li> <li>Deep Learning<ul> <li>Virtual Machines</li> <li>DonkeyCar: running on student host computer</li> <li>DonkeySim: running on student host computer</li> </ul> </li> <li>CBT/e-book<ul> <li>Embedded Linux</li> <li>Python</li> <li>Concepts of ROS2</li> </ul> </li> </ul>"},{"location":"course-overview/syllabus/#assignments-due","title":"Assignments Due","text":"<ul> <li>Begin designing robot<ul> <li>Electronics mount plate</li> <li>3D print camera mount</li> <li>3D print single board computer case</li> </ul> </li> <li>DonkeyCar DonkeySim<ul> <li>3 autonomous laps running on student host computer (due Tuesday of Week 2)</li> </ul> </li> <li>Linux and Python traning modules</li> <li>ROS2 Basics in 5 Days (Python) Section: Introduction</li> </ul>"},{"location":"course-overview/syllabus/#week-2","title":"Week 2","text":""},{"location":"course-overview/syllabus/#lecture-and-lab_1","title":"Lecture and Lab","text":"<ul> <li>Team members review and adjust. Let\u2019s lock the teams. Starting this week it will be very hard for people to join and catch.</li> <li>Deep Learning<ul> <li>Review of Virtual Machines and Host Machines<ul> <li>We use a virtual machine image from a hypervisor company called VMware</li> <li>The instructions to use our virtual machine is here</li> </ul> </li> <li>DonkeyCar - running on students host computer</li> <li>DonkeySim - running on the external server</li> </ul> </li> <li>Embedded Linux on a low power single board computer (SBC)<ul> <li>How can we install software without a computer monitor, keyboard, and mouse connected to an embedded computer (single board computer)?</li> </ul> </li> <li>Installing the software into the SBC</li> <li>Jetson Nano Single Board Computer (SBC) Hands-on<ul> <li>Remote access without a monitor, keyboard, mouse<ul> <li>Initial connection using a USB cable</li> <li>Connect to a local WiFi access point e.g., UCSDRoboCar</li> </ul> </li> <li>Multi-user on a low power SBC</li> <li>User Security</li> <li>Installing software using Secure Shell (SSH)</li> <li>Remote Desktop</li> </ul> </li> <li>Robot Components and Electronics</li> <li>GPS Based Navigation<ul> <li>GNSS (GPS) 3D Localization</li> <li>RTK GNSS Error Correction<ul> <li>Base Station</li> <li>Services<ul> <li>Open source</li> <li>Paid services</li> </ul> </li> </ul> </li> </ul> </li> <li>Robot design completed, and major components in place<ul> <li>Completed<ul> <li>Electronics mount plate</li> <li>3D printed camera mount</li> <li>3D printed case for the single board computer (SBC)</li> </ul> </li> <li>Incorporate<ul> <li>GNSS unit and antenna</li> <li>Electronics wiring</li> </ul> </li> </ul> </li> <li>CBT / e-book<ul> <li>ROS2 Basics: Topics, Launch files</li> </ul> </li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_1","title":"Assignments Due","text":"<ul> <li>Donkey Car DonkeySim 3 autonomous laps - sim running on the external server (due Thursday of Week 2)</li> <li>Basic software setup on Jetson (Jetpack, WiFi, hostname, DonkeyCar, etc.) (due Tuesday of Week 3)</li> <li>Robot components ready (Thursday of Week 3)<ul> <li>Mechanical Components</li> <li>Electrical Components</li> <li>Software - Linux, Jetpack, OpenCV GPU Accelerated, DonkeyCar </li> <li>Deep Learning laps</li> <li>DonkeyCar GNSS Navigation</li> </ul> </li> <li>ROS2 Basics in 5 Days (Python) Sections: Basic Concepts, Topics</li> </ul>"},{"location":"course-overview/syllabus/#week-3","title":"Week 3","text":""},{"location":"course-overview/syllabus/#lecture-and-lab_2","title":"Lecture and Lab","text":"<ul> <li>UCSD\u2019s SuperComputer GPU Cluster Deep Learning acceleration</li> <li>Hands-on GPS/GNSS Based Navigation - putting all together - In font of EBU I<ul> <li>3 outdoors autonomous laps using GNSS</li> </ul> </li> <li>3 Autonomous Laps Deep Learning on EBUII outdoor track</li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_2","title":"Assignments Due","text":"<ul> <li>3 Autonomous Laps GNSS / GPS - EBU I (Tuesday of Week 4)</li> <li>3 Autonomous Laps Deep Learning - EBU II (Thursday of Week 4)</li> <li>ROS2 Basics in 5 Days (Python) Section: Services</li> </ul>"},{"location":"course-overview/syllabus/#week-4","title":"Week 4","text":""},{"location":"course-overview/syllabus/#lecture","title":"Lecture","text":"<ul> <li>Introduction to Docker and Git</li> <li>Introduction to UCSD ROS 2 Robocar Framework </li> <li>Demo of Python Camera Based Navigation</li> <li>Introduction to Class Final Project requirements</li> <li>Introduction to Project Management</li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_3","title":"Assignments Due","text":"<ul> <li>Class Final Project Proposal (Tuesday Week 5)</li> <li>3 Autonomous Laps ROS2 (Thursday Week 5)</li> <li>ROS2 Basics in 5 Days (Python) Section: Actions</li> <li>ROS2 Guidebook</li> </ul>"},{"location":"course-overview/syllabus/#week-5","title":"Week 5","text":""},{"location":"course-overview/syllabus/#lecture_1","title":"Lecture","text":"<ul> <li>Neural Network</li> <li>Car Dynamics</li> <li>Final Project Status Update</li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_4","title":"Assignments Due","text":"<ul> <li>Weekly progress report on final project</li> </ul>"},{"location":"course-overview/syllabus/#weeks-6-10","title":"Weeks 6 - 10","text":""},{"location":"course-overview/syllabus/#lecture_2","title":"Lecture","text":"<ul> <li>As needed/per request<ul> <li>Suggested topics:<ul> <li>Filtering/state estimation</li> <li>Path planning: GPS waypoint, LiDAR, SLAM</li> <li>PID control</li> <li>ROSBAGS, rviz, rqt</li> </ul> </li> </ul> </li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_5","title":"Assignments Due","text":"<ul> <li>Weekly presentation progress report on final project</li> <li>Bonus: ROS2 NAV Course</li> </ul>"},{"location":"guidebooks/30-robocar-jetson/","title":"UCSD Robocar Jetson Nano Configuration","text":""},{"location":"guidebooks/30-robocar-jetson/#introduction","title":"Introduction","text":"<p>The NVIDIA Jetson Nano in this document is also referred as JTN.</p> <p>To learn more about the JTN, you can check out the NVIDIA documentation sources Getting Started, Jetson Download Center, and the documentation for usage on DonkeyCar.</p> <p>!!! info \"     In general for this course if you are using a computer with a Linux distribution like Ubuntu you will have an easier time installing the necessary Artificial Intelligence framework. Linux uses a file format that is not easily read in an Apple MacOS or MS Windows computer.  If you need to modify files in the \\(\\mu\\)SD card formatted for Linux, e.g., a disk partition that uses Ext4 format, you should ask a colleague first if they have a Linux PC, then Tutors or TA for help.</p> <p>Every Team is expected to try installing the necessary software based on these instructions using the \\(\\mu\\)SD image, not the recovery image.  And every member of the team needs to participate during the lab embedded linux hands-on exercise. The same SBC (JTN) can be used at the class by several users without the need to connect a monitor and a keyboard to it. It is part of your learning in the course to get familiar with Embedded Linux, Head-Less Single Board Computers (SBC) like the JTN, terminal running in a PC, SSH, and remotely installing software in a SBC.</p> <p>If you run out of time, based on instructor\u2019s set deliverables, or your \\(\\mu\\)SD card gets corrupted, you can use the recovery \\(\\mu\\)SD card image that has all the software pre-installed. On the 10-UCSD Robocar ECE &amp; MAE 148 document there is a link to a recovery \\(\\mu\\)SD card image.</p>"},{"location":"guidebooks/30-robocar-jetson/#flashing-microsd-card-for-the-jetson-nano","title":"Flashing microSD card for the Jetson Nano","text":"<p>From your PC let's prepare (flash) the microSD card (\\(\\mu\\)SD card). Make sure your computer can access the Internet. If you are using one of our WiFi Access Points in one of the labs, or at one of the tracks, the first device that connects to the WiFi Access point and try to access the Internet, will need to accept the UCSD Wireless Visitor Agreement, just like when you are connecting to UCSD\u2019s Visitor WiFi. In fact you are basically connecting to the Internet via the UCSD\u2019s Visitor WiFi. </p>"},{"location":"guidebooks/30-robocar-jetson/#software-installation","title":"Software Installation","text":"<p>Etcher can use Zipped files, you don\u2019t need to Unzip the image file if you are using Etcher. Ignore if your Windows or Mac computer tells you it can not ready the \\(\\mu\\)SD card. The \\(\\mu\\)SD card will be used on the JTN running Linux. It uses a file format that Windows and Mac don\u2019t know about unless you use specialized software to do that. You don\u2019t need it. If you are using Linux or MacOS command lines to write the disk image to a \\(\\mu\\)SD card, you may need to extract the file first.</p> <p>Download the UCSD Jetson Nano Developer Kit \\(\\mu\\)SD Card Image here.</p> <p>Once you have the image, install Etcher to be able to write the disk image to a microSD card here.</p> <p>Using the image we are providing as a starting point has some advantages with some pre-configuration. For example, it won't require a monitor and a keyboard connected to the JTN to get you started.</p>"},{"location":"guidebooks/30-robocar-jetson/#writing-an-image-to-a-musd-card","title":"Writing an image to a \\(\\mu\\)SD card","text":"<p>Connect a \\(\\mu\\)SD adapter to your PC.</p> <ol> <li>Insert the provided \\(\\mu\\)SD card (64 gigabytes) into the \\(\\mu\\)SD adapter.</li> <li>Install and run Etcher.</li> <li>Start Etcher, choose the Zipped file with the Disk Image you downloaded, pay attention when choosing the drive with the \\(\\mu\\)SD card on it (e.g., 64 gigabytes).</li> <li>Write the image to \\(\\mu\\)SD card.</li> <li>Eject the \\(\\mu\\)SD card from your PC first using the procedure for your PC Operating System (e.g., eject drive) </li> <li>Then insert the \\(\\mu\\)SD card into the JTN \\(\\mu\\)SD card slot. Note this is a push-in-to-lock and push-in-to-unlock the \\(\\mu\\)SD card. Please do not pull the \\(\\mu\\)SD card out of the slot before unlocking it, otherwise you may damage your JTN and or \\(\\mu\\)SD card.</li> </ol>"},{"location":"guidebooks/30-robocar-jetson/#powering-on-the-jtn","title":"Powering on the JTN","text":"<p>The preferable way to power the JTN is with the provided 5V 4A power supply; it has a barrel connector that plugs into the JTN. You need to place a jumper connector at J48, there is a text label close to it that says ADD JUMPER TO DISABLE USB PWR. </p> <p>For the initial configuration you don\u2019t have to use the 5V 4A power supply. If you are not using the provided power supply for the initial configuration, you need to remove the jumper located above the JNT power connectors to allow it to be powered by the uUSB cable. Please save the jumper. You can leave it in place connected to one of the pins of J48.</p> <p>Power on the JTN by connecting the power supply to a power outlet and the barrel jack into the barrel port on the Jetson. </p> <p>Note</p> <p>Give the Jetson a minute or two to boot and load its software and the fan may not work until you install the software later in these instructions.</p>"},{"location":"guidebooks/30-robocar-jetson/#wired-communication-with-the-jtn","title":"Wired communication with the JTN","text":"<p>We will use the command line to access the Jetson Nano (JTN) using a secure shell (SSH). Don\u2019t worry, if you are not familiar with terminals and using command lines, you will master it in this class. Mastering these will be a good skill to have and another mention in your resume.</p> <p>The Linux OS running on the JTN enables two protocols of communication: Serial Communication and SSH. Both use a local network interface with TCP/IP over the USB cable. By using a micro USB cable between the JTN and the PC, you can communicate with the JTN. On the PC, you need to use software to enable serial communication or SSH. e.g., Serial Communication - screen on Linux or MacOS. CoolTerm is another option that runs on Linux, MacOS, and MS Windows. SSH is natively supported on Linux and MacOS. MS Windows 10 or later supports SSH too.</p> <p>Note</p> <p>Your JTN should have a WiFi / Bluetooth network interface and antennas already installed on it. If not, please contact the Tutors or TAs.</p>"},{"location":"guidebooks/30-robocar-jetson/#ssh-communication-via-usb-connection","title":"SSH Communication (via USB connection)","text":"<p>Connect a \\(\\mu\\)USB cable between the JTN and your PC. Then, from a terminal, run:</p> <pre><code>ssh jetson@192.168.55.1\n</code></pre> <p>Or:</p> <pre><code>ssh jetson@ucsdrobocar-xxx-yy.local\n</code></pre> <p>The default UserID and password is:</p> <p>Username: <code>jetson</code>  Password: <code>jetsonucsd</code></p>"},{"location":"guidebooks/30-robocar-jetson/#serial-communication","title":"Serial Communication","text":"<p>If needed, install a software called <code>screen</code> on your host or virtual Linux computer and/or update it.</p> <pre><code>sudo apt-get install screen\nsudo apt-get update screen\n</code></pre> <p>Or you can install coolterm, or use any terminal emulator software you have. On my computer the JTN plugged in using the uUSB cable shows up as <code>/dev/ttyACM1</code>. Log-in info when doing a serial connection is the same as the SSH connection</p> <p>The command line to connect in a Linux terminal is:</p> <pre><code>screen /dev/ttyACM1\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#wireless-communication-with-the-jtn","title":"Wireless communication with the JTN","text":"<p>After connecting to the JTN via USB connection, let's configure the JTN to connect to a WiFi Access Point. Using SSH via USB connection or using the serial communication software log into the JTN.</p> <p>First, let's make sure the network service is running.</p> <pre><code>sudo systemctl start networking.service\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#accessing-wifi-networks","title":"Accessing WiFi networks","text":"<p>To list the available WiFi networks:</p> <pre><code>sudo nmcli device wifi list\n</code></pre> <p>If the WiFi Access point that you want to connect is not initially listed try rescan to refresh the list. If after several tries the network still does not show, try rebooting by typing <code>sudo reboot</code>. After your machine has restarted:</p> <pre><code>sudo nmcli device wifi rescan\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#connecting-to-a-wifi-network","title":"Connecting to a WiFi network","text":"<p>We have bridged WiFi access points. Try to stay in the 5GHz network. The command to connect he JTN to an access point is</p> <pre><code>sudo nmcli device wifi connect &lt;ssid_name&gt; password &lt;password&gt;\n</code></pre> <p>Here are the WiFi access points we use:</p> <pre><code>ssid=\"UCSDRoboCar5GHz\"\npassword=\"UCSDrobocars2018\"\nssid=\"UCSDRoboCar\"\npassword=\"UCSDrobocars2018\"\nssid=\"SD-DIYRoboCar5GHz\" \npassword=\"SDrobocars2017\"\nssid=\"SD-DIYRoboCar\"\npassword=\"SDrobocars2017\"\n</code></pre> <p>So, to connect, an example command could look like:</p> 5GHz<pre><code>sudo nmcli device wifi connect UCSDRoboCar5GHz password UCSDrobocars2018\n</code></pre> 2.4GHz<pre><code>sudo nmcli device wifi connect UCSDRoboCar password UCSDrobocars2018\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#get-ip-address-of-jtn","title":"Get IP address of JTN","text":"<p>After connecting the JTN to a WiFi Access Point, let's find out the IP address the JTN is getting from the network so we can connect to it remotely. </p> <p>In a terminal:</p> <pre><code>ifconfig\n</code></pre> <p>You should see some output that contains the following line:</p> <pre><code>inet 192.168.222.167\n</code></pre> <p>Note</p> <p>Your IP address may be slightly different. It's important to use the corresponding <code>inet</code> IP address output onto your screen when configuring your JTN.</p> <p>In the future, to connect to a new Access Point you may have to repeat these steps using a \\(\\mu\\)USB cable connected between your PC and the JTN.</p> <p>Helpful Hint: You can add your phone as an Access Point so you always have a backup connection to your JTN. If you have your phone with you, you can connect to the JTN and then add another WiFi using SSH.</p>"},{"location":"guidebooks/30-robocar-jetson/#ssh-communication-via-wifi","title":"SSH communication (via WiFi)","text":"<p>Connect your PC to the JTN using WiFi (using example IP address from above) then enter the username and password.</p> <pre><code>ssh jetson@192.168.222.167\nUsername: jetson\nPassword: jetsonucsd\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#update-wifi-power-settings","title":"Update WiFi power settings","text":"<p>Lets turn off the power saving for the WiFi device.</p> <p>If your SSH connection is slow or lagging, make sure you have the power saving on the WiFi disabled.</p> <pre><code>sudo iw dev wlan0 set power_save off\n</code></pre> <p>Now let's install a small text editor called <code>nano</code>.</p> <pre><code>sudo apt-get update\nsudo apt-get install nano\n</code></pre> <p>Lets make the change on the WiFi power saving settings persistent. We need to edit a file. </p> <pre><code>sudo nano /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf\n</code></pre> <p>In the file, change</p> <pre><code>wifi.powersave = 3\n</code></pre> <p>to </p> <pre><code>wifi.powersave = 2\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#system-settings","title":"System Settings","text":"<p>Now lets change the name of the JTN (host name) and password of the JTN. We will configure the name of the JTN based on your Class <code>xxx</code> and Team <code>yy</code>. Hostname <code>ucsdrobocar-xxx-yy</code> where <code>xxx-yy</code> is your class and team number, ex: <code>148-05</code>, <code>190-01</code>, <code>190-TA1</code>, <code>190-TA2</code> Example: <code>ucsdrobocar-148-05</code> for Team 5</p> <p>To see your current computer information:</p> <pre><code>hostnamectl\n</code></pre> <p>insert screenshot</p>"},{"location":"guidebooks/30-robocar-jetson/#changing-your-hostname","title":"Changing your hostname","text":"<p>Replace <code>xxx-yy</code> as shown above with your class name and team number.</p> <pre><code>sudo hostnamectl set-hostname ucsdrobocar-xxx-yy\n</code></pre> <p>Then there is one more place to change the hostname:</p> <pre><code>sudo nano /etc/hosts\n</code></pre> <p>insert screenshot</p> <p>Change the name <code>jetson</code> here or any other name it may have for your JTN, for example</p> <pre><code>127.0.0.1   localhost\n127.0.1.1   ucsdrobocar-xxx-yy\n</code></pre> <p>When making edits using <code>nano</code>, to save the file, first press Ctrl+O to write out your changes. Press Enter to save them under the same filename, and then press Ctrl+X to exit.</p>"},{"location":"guidebooks/50-vm-guide/","title":"Virtual Machine with DonkeyCar/DonkeySim AI Simulator","text":""},{"location":"guidebooks/50-vm-guide/#initial-setup","title":"Initial Setup","text":""},{"location":"guidebooks/50-vm-guide/#download-and-install","title":"Download and Install","text":"<p>Download VMware Player (Windows and Linux), Fusion Player (MacOS). Not working for Mac M1 or M2 yet.</p> <p>A VMware virtual machine image can be downloaded from here. Download and unzip it onto your host computer.</p> <p>Once unzipped the virtual machine image will take about 40 GB of disc space. If needed, clear some space from your computer, i.e. removing videos or other large media files. Alternatively, use a USB external drive if needed.</p> <ul> <li>If you unzip the image onto an external USB, you will have to ensure the USB is connected each time you run the virtual machine on VMware.  </li> <li>If VMware Player asks you \"Did you copy or move these files?\" select \"I copied them.\"</li> </ul> <p>You need a host machine with at least 8 GB RAM. The virtual machine is configured by default to use 5 GB RAM. If your host machine has 16 GB or more, you will need to configure your settings in VMware to use 8 GB. You can change this setting in your virtual hardware configuration while the virtual machine is not running.</p>"},{"location":"guidebooks/50-vm-guide/#initial-boot-of-vm","title":"Initial Boot of VM","text":"<p>Start the virtual machine using the VMware player (Windows, Linux) or Fusion Player (MacOS). - If needed to boot, enable Virtualization on your BIOS/EFI - On the VMware Player machine configuration, disable \"Optimization for Virtualization.\" - Example of settings for Fusion Player on Mac:</p>"},{"location":"guidebooks/50-vm-guide/#log-into-vmware-virtual-machine","title":"Log into VMware virtual machine","text":"<p>Username: <code>ucsd</code>  Password: <code>UcsdStudent</code></p>"},{"location":"guidebooks/50-vm-guide/#copypaste-capabilitites","title":"Copy/Paste Capabilitites","text":"<p>If cutting and pasting from your host computer to your virtual machine is not working, open a terminal in the Linux VM and run the following:</p> <pre><code>sudo apt-get autoremove open-vm-tools\nsudo apt-get install open-vm-tools-desktop\nsudo reboot\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#connecting-game-controller","title":"Connecting Game Controller","text":"<p>Using a game controller will be the desirable way to control your DonkeyCar simulation. Examples of compatible game controllers include PS3, PS4, Xbox, Logitech F710.</p> <p>The controller needs to be connected to the host computer using USB cable, Bluetooth or USB dongle.</p> <p>Once the controller is connected to the host, the VMware Player should ask you where you want to connect the game controller ('Connect to the host' or 'Connect to a virtual machine'). Select the UCSD-Ubuntu machine.</p> <p>The pop-up window should appear as one of the following:</p> <p>image here</p> <p>If you don't see this option, reconnect the controller.</p> <p>To verify that the virtual machine can see the game controller, check the input for <code>js0</code>.</p> <ul> <li>Open a terminal in Linux machine and run the following command.</li> </ul> <pre><code>ls /dev/input\n</code></pre> <p>You should see:</p> <pre><code>by-id    event0  event2  event4  event6  js0   mouse0  mouse2\nby-path  event1  event3  event5  event7  mice  mouse1  mouse3\n</code></pre> <p>Once you confirm that you can see <code>js0</code>, let's test the joystick controls.</p> <ul> <li>From a terminal:</li> </ul> <pre><code>sudo apt-get update\nsudo apt-get install -y jstest-gtk\njstest /dev/input/js0\n</code></pre> <p>image of joystick controls</p> <p>Move/press the game controller buttons to test which buttons trigger the corresponding input values, i.e. X, O, Triangle, Select/Share, Start/Options.</p> <p>Jack, what if my game controller works with the <code>jtest /dev/input/js0</code> but is not working with the DonkeyCar?</p> <p>Not to worry, there is a way to build a custom file for your game controller with the controls we use most.</p>"},{"location":"guidebooks/50-vm-guide/#custom-controller-if-neededdesired","title":"Custom controller (if needed/desired)","text":"<p>If you have a controller that is not listed above, or you are having trouble getting your controller to work, or you simply want to map your controller differently, see the Custom Game Controller documentation on DonkeyCar.</p> <p>To discover or modify the button and axis mappings for your controller, you can use the Joystick Wizard via the command line. </p> <p>The Joystick Wizard will write a custom controller named <code>my_joystick.py</code> to your <code>mycar</code> folder. To use the custom controller, set <code>CONTROLLER_TYPE=\"custom\"</code> in your <code>myconfig.py</code>.</p> <ul> <li>From a terminal, navigate to your <code>mycar</code> directory.</li> </ul> <pre><code>cd ~/projects/d4_sim\n</code></pre> <p>This should be the directory with all of your Python configuration files, i.e. <code>manage.py</code>, <code>train.py</code>, <code>myconfig.py</code>.</p> <ul> <li>First, make sure the OS can access your device. The utility <code>jtest</code> can be usefull here. If needed:</li> </ul> <pre><code>sudo apt install joystick\n</code></pre> <p>You must pass this utility the path to your controller's device. Typically this will be <code>/dev/input/js0</code>. However, if it is not, you must find the correct device path and provide it to the utility. You will need this for the <code>createjs</code> command as well.</p> <ul> <li>From the current directory you have just navigated to, run the command</li> </ul> <pre><code>donkey createjs\n</code></pre> <p>This will create a file named <code>my_joystick.py</code> in your <code>/d4_sim</code> folder, next to your <code>manage.py</code>.</p> <p>Modify <code>myconfig.py</code> to use your new controller.</p> <pre><code>atom myconfig.py\n</code></pre> <p>Find the line below and modify it as:</p> <pre><code>CONTROLLER_TYPE=\"custom\"\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#donkeycar-ai-framework","title":"DonkeyCar AI Framework","text":"<p>DonkeyCar AI Framework Explained</p>"},{"location":"guidebooks/50-vm-guide/#how-to-launch-the-simulator","title":"How to launch the simulator","text":"<p>To start the DonkeySim:</p> <ul> <li>Use the File Explorer to navigate to <code>~/projects/DonkeySimLinux</code>.  </li> <li>Double click the file <code>donkey_sim.x86_64</code> to execute.  </li> <li>You should now see the DonkeySim ready for use.</li> </ul> <p>Depending on the track we will be racing on, you need to train on the track that will be used during the race in order to successfully race against other people.</p> <p>Some tracks you will see are <code>donkey-circuit-launch-track-v0</code>, <code>donkey-warren-track-v0</code>, <code>donkey-mountain-track-v0</code>.</p> <p>Specify track for current quarter</p>"},{"location":"guidebooks/50-vm-guide/#customize-your-virtual-robot-racer","title":"Customize your virtual robot racer","text":"<ul> <li>From a terminal:</li> </ul> <pre><code>cd ~/projects/d4_sim\natom myconfig.py\n</code></pre> <p>This time you will pull up the <code>myconfig.py</code> file to investigate and edit more in-depth.</p> <p>In your <code>myconfig.py</code> you may modify the following variables</p> <pre><code>GYM_CONF[\"racer_name\"] = \"UCSD-148-YourName\"\nGYM_CONF[\"country\"] = \"USA\"\nGYM_CONF[\"bio\"] = \"Something_about_you, ex: Made in Brazil\"\n</code></pre> <p>in addition to the <code>car_name</code> variable in the following line.</p> <pre><code>GYM_CONF = { \"body_style\" : \"car01\", \"body_rgb\" : (255, 205, 0), \"car_name\" : \"UCSD-148-YourName\", \"font_size\" : 30}\n</code></pre> <p>You can also change the color of the car to one of UCSD's colors, blue or gold. You can see how by examining the commented lines in the demo file below.</p> <p>An example <code>myconfig.py</code> file:</p> myconfig.py<pre><code># 04Jan22\n# UCSD mods to make easier for the UCSD students to use the Donkey-Sim\n# the following uncommented lines where copied here from the body of myconfig.py below\nDONKEY_GYM = True\n# DONKEY_SIM_PATH = \"remote\"\nDONKEY_SIM_PATH = \"/home/ucsd/projects/DonkeySimLinux/donkey_sim.x86_64\"\n# DONKEY_GYM_ENV_NAME = \"donkey-warren-track-v0\"\nDONKEY_GYM_ENV_NAME = \u201cdonkey-mountain-track-v0\u201d\n# UCSD yellow color in RGB = 255, 205, 0\n# UCSD blue color in RGB = 0, 106, 150\nGYM_CONF = { \"body_style\" : \"car01\", \"body_rgb\" : (255, 205, 0), \"car_name\" : \"UCSD-148-YourName\", \"font_size\" : 30} # body style(donkey|bare|car01) body rgb 0-255\nGYM_CONF[\"racer_name\"] = \"UCSD-148-YourName\"\nGYM_CONF[\"country\"] = \"USA\"\nGYM_CONF[\"bio\"] = \"Something_about_you, ex: Made in Brazil\"\n#\n# SIM_HOST = \"donkey-sim.roboticist.dev\"\nSIM_ARTIFICIAL_LATENCY = 0\nSIM_HOST = \"127.0.0.1\"  \n# when racing on virtual-race-league use host \"roboticist.dev\"\n# SIM_ARTIFICIAL_LATENCY = 30\n# when training on the host machine, \n# set artificial latency to the value \n# when you ping roboticist.dev. When racing on \n# virtual-race league, use 0 (zero)\n#\n# When racing, to give the ai a boost, configure these values.\nAI_LAUNCH_DURATION = 3     # the ai will output throttle for this many seconds\nAI_LAUNCH_THROTTLE = 1     # the ai will output this throttle value\nAI_LAUNCH_KEEP_ENABLED = True     # when False (default) you will need to hit the \n# AI_LAUNCH_ENABLE_BUTTON for each use. \n# This is safest. When this True, is active on each trip into \"local\" ai mode.\n#\n# JOYSTICK\n# When using a joystick modify these by uncommenting USE_JOYSTICK_AS_DEFAULT = True\n#\n# USE_JOYSTICK_AS_DEFAULT = True  \n# when starting the manage.py, when True, will not require a --js option to use the joystick\nJOYSTICK_MAX_THROTTLE = 1.0      # this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don't need the full speed available.\nJOYSTICK_STEERING_SCALE = 0.8    # some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\nAUTO_RECORD_ON_THROTTLE = True   # if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\nJOYSTICK_DEADZONE = 0.2      # when non zero, this is the smallest throttle before recording triggered.\n# #Scale the output of the throttle of the ai pilot for all model types.\nAI_THROTTLE_MULT = 1.0     # this multiplier will scale every throttle value for all output from NN models\n#\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#get-latency-from-remote-server-roboticistdev","title":"Get latency from remote server <code>roboticist.dev</code>","text":"<p>First, ping the remote server and take note of the average ping time, i.e. 30 ms.</p> <pre><code>ping donkey-sim.roboticist.dev\n</code></pre> <p>insert image of ping results</p> <p>In this example, since I am in the same network as <code>donkey-sim.roboticist.dev</code> my ping time is much smaller, around 0.5 ms. Assuming you are somewhere in the USA, you should get between 20-60 ms.</p> <p>Write this value into your <code>myconfig.py</code> file as shown above.</p> <pre><code>SIM_ARTIFICIAL_LATENCY = 30\n</code></pre> <p>Remember: This step is crucial to train your car for remote racing when you are not on the same network in the future. </p>"},{"location":"guidebooks/50-vm-guide/#collecting-data","title":"Collecting data","text":"<p>For training your model, we will do a \"behavioral cloning\" AI.</p> <p>Begin by driving the robot in the local simulator, aka the simulator on your virtual machine that is running on your host.</p> <p>If you have not already, activate the donkey virtual environment.</p> <pre><code>conda activate donkey\n</code></pre> <p>If you have activated the venv correctly, your terminal line should now look something like this:</p> <pre><code>(donkey) ucsd@ucsd-virt-ub:~/$\n</code></pre> <p>Navigate to your DonkeyCar directory.</p> <pre><code>cd ~/projects/d4_sim\n</code></pre> <p>Now let's drive the robot to collect data automatically. In your terminal, run:</p> <pre><code>python manage.py drive\n</code></pre> <p>You should see the following output.</p> <pre><code>You can now go to http://localhost:8887 to drive your car.\nStarting vehicle at 20 Hz\n</code></pre> <p>Open a web browser like Chrome (or Firefox on Linux).</p> <pre><code>http://localhost:8887\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#driving-your-robot","title":"Driving your robot","text":"<p>To control your car, you can either use the controller after configuring it following the steps above, or you can drive with your keyboard and mouse from the web browser.</p> <p>Some helpful settings for your joystick to improve controllability are outlined in the <code># JOYSTICK</code> section of <code>myconfig.py</code>.</p> <pre><code># JOYSTICK\nUSE_JOYSTICK_AS_DEFAULT = True   # when starting the manage.py\nJOYSTICK_MAX_THROTTLE = 1.0      # this scalar is multiplied with the -1 to 1\nJOYSTICK_STEERING_SCALE = 0.7    # some people want a steering that is less sensitive\nAUTO_RECORD_ON_THROTTLE = True   # if true, will record whenever throttle is not zero\nCONTROLLER_TYPE='F710'           # (ps3|ps4|xbox|nimbus|wiiu|F710|rc3|MM1|custom)\nJOYSTICK_DEADZONE = 0.0          # when non zero, this is the smallest throttle before recording triggered\n</code></pre> <p>Hint: if your trained model is having trouble with accuracy when going around curves, try setting <code>AUTO_RECORD_ON_THROTTLE = False</code> and engage your throttle manually. This way there will not be any gaps in recording data.</p> <p>Some advantages to using a game controller over your mouse/keyboard:</p> <ul> <li>Delete 100 last data points (at 20 Hz, this is equivalent to the last 5 seconds of driving data)<ul> <li>On PS3/PS4 this is usually \"Triangle\" button</li> <li>On Logitech F710 this is usually \"Y\"</li> <li>If you miss a turn or accidentally drive your vehicle into a wall, just remove that 5 seconds of recording and keep going</li> </ul> </li> <li>Emergency Stop</li> <li>Easily switch operation modes from AI to manual </li> </ul> <p>Practice for a while and don't worry about not driving well initially. We'll show you how to delete all the data you've collected in one run so you can start with a clean set later.</p> <p>We will be training the AI model in increments of approximately 20 laps, so count your laps as feasible.</p> <p>To stop the DonkeyCar, use <code>Ctrl+C</code> like other Python code you've used so far.</p>"},{"location":"guidebooks/50-vm-guide/#deleting-data","title":"Deleting data","text":"<p>You can delete your entire data directory to start fresh if you'd like. This can be done with basic Linux commands you've recently learned.</p> <p>Where is the data stored? Take a guess.</p> <pre><code>cd ~/projects/d4_sim\nls\n</code></pre> <p>Inside your directory you should see a folder called <code>/data</code>. Let's clean it up and start fresh.</p> <pre><code>rm -rf data\n</code></pre> <p>Then, create the directory again.</p> <pre><code>mkdir data\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#training-and-testing","title":"Training and testing","text":"<p>OK, so you've recorded 20 laps of driving data. Now what?</p> <p>It's time to train your model using all the data by giving it a name when running the <code>train.py</code> command. From your <code>/d4_sim</code> directory, run the command:</p> <pre><code>python train.py --model=models/14mar24_sim_160x120_20.h5 --type=linear --tub=./data\n</code></pre> <p>By executing this command with the <code>--model</code> option, you are telling the framework to train a new model called <code>14mar24_sim_160x120_20.h5</code> with the data you've collected, with the <code>20</code> tag at the end indicating how many laps you're using to train with.</p> <p>Depending on your connection speed and the amount of laps you've recorded, this could take a while. Once your model has been trained, let's test it.</p> <pre><code>python manage.py drive --model=models/14mar24_sim_160x120_20.h5 --type=linear\n</code></pre> <p>This should open up the simulator as before. Press \"Select\" twice on your controller to start the AI model (or whichever button corresponds with your controller to activate AI drive).</p>"},{"location":"guidebooks/50-vm-guide/#does-your-model-work","title":"Does your model work?","text":"<p>A lot of things may have just happened.</p> <ul> <li>Does your car go berserk? Don't worry, there's a few things you can try.  <ul> <li>Add another 20 laps and train again</li> <li>Go into your <code>myconfig.py</code> and modify the sections of the code that configure the driving behavior of your AI model.  <ul> <li>This could consist of increasing or decreasing the AI throttle boost, changing the max throttle, changing the throttle multiplier, etc.</li> </ul> </li> </ul> </li> <li>Does your car drive well but lose it around the corners?  <ul> <li>Check to make sure your <code>AUTO_RECORD_ON_THROTTLE</code> is set to <code>False</code>. This way you won't lose data when you release the gas around tight corners, and you can also manually choose when you want to record your driving and when to stop.</li> </ul> </li> </ul> <p>While driving your robot to collect data, it is a good idea to keep the terminal visible so you can see whether you are recording data. Sometimes you may accidentally turn recording off while you are driving and you may not know it.</p> <p>As you continue adding data by driving more laps manually, you can rename the model as you train. For example, if you add 20 more laps and train again, you may choose to change the <code>20</code> tag to <code>40</code> on your next train.</p> <pre><code>python train.py --model=models/14mar24_sim_160x120_40.h5 --type=linear --tub=./data\n</code></pre> <p>Remember to run this from your <code>/d4_sim</code> directory for it to work</p> <p>Now let's test your model again with the new data.</p> <pre><code>python manage.py drive --model=models/14mar24_160x120_40.h5 --type=linear\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#training-on-a-specific-tub","title":"Training on a specific tub","text":"<p>You may also choose to drive your model and store data in a different tub each time you add new laps.</p> <pre><code>cd ~/projects/d4_sim\nmkdir -p data/tub_2\npython manage.py drive --model=models/14mar24_160x120_40.h5 --type=linear --tub=data/tub_2\n</code></pre> <p>This instance creates a new directory in the <code>/data</code> directory, then initiates your simulator and points to the new tub for your driving data to be recorded.</p> <p>Add 20 more laps. Then, to train with your new data and transfer the training to your previous model, run the following command.</p> <pre><code>python train.py --tub=data/tub_1 --transfer=models/previous_model.h5 --model=models/new_model.h5\n</code></pre> <p>For this example, the command would be</p> <pre><code>python train.py --tub=data/tub_1 --transfer=models/14mar24_sim_160x120_40.h5 --model=models/14mar24_160x120_40.h5\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#ucsd-gpu-cluster","title":"UCSD GPU Cluster","text":"<p>Please do not use the GPU cluster until you demonstrate training on your local machine first. It is part of your deliverables for the class.</p>"}]}