{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>Welcome to the course documentation page for ECE/MAE 148: Intro to Autonomous Vehicles at University of California, San Diego.</p>"},{"location":"#what-youll-find-here","title":"What you'll find here","text":"<p>This page will contain all of the necessary documentation for your 10-week class experience, including step-by-step documentation for every component of your individual and group deliverables, troubleshooting, final project documentation, and reference examples.</p>"},{"location":"#about-this-course","title":"About this course","text":"<p>ECE/MAE 148 is a highly popular technical elective course for Engineering students across both disciplines. In this class you will receive invaluable hands-on experience in project-based learning as you work in teams to design, build and program a 1:10 scale robot vehicle capable of performing different tasks while autonomously driving, from simulation to deployment on your real, physical car. This won't be your usual classroom experience in that a majority of your learning will come from getting your hands dirty as opposed to written assignment exercises. </p> <p>In 148, your success is contingent upon your willingness to put in the minimum time expected for a 4-unit class, and then some. For example, if your team consists of 4 members, that's 40 hours per week in combined team efforts to reach your objectives. In return, you will walk away with a set of real, crucial skills to set you apart from everyone else when it comes time for graduate school or career applications; in short, it's well worth it, and you'll have a lot of fun doing it.</p>"},{"location":"course-deliverables/group/","title":"Group Assignments","text":""},{"location":"course-deliverables/group/#physical-robot","title":"Physical Robot","text":"<p>As the course progresses your group will be required to present physical evidence of your robot's progress, including videos of your physical robot completing the given objectives.</p>"},{"location":"course-deliverables/group/#hardware","title":"Hardware","text":"<p>TODO</p>"},{"location":"course-deliverables/group/#donkeycar-computer-vision","title":"DonkeyCar: Computer Vision","text":"<p>TODO: link documentation for physical autonomous laps in DonkeyCar</p>"},{"location":"course-deliverables/group/#donkeycar-gps-navigation","title":"DonkeyCar: GPS Navigation","text":"<p>TODO: link documentation for GPS laps in Donkey</p>"},{"location":"course-deliverables/group/#ros2","title":"ROS2","text":""},{"location":"course-deliverables/group/#line-following-laps","title":"Line-Following Laps","text":"<p>TODO: line following docs</p>"},{"location":"course-deliverables/group/#lane-following-laps","title":"Lane-Following Laps","text":"<p>TODO: lane following docs</p>"},{"location":"course-deliverables/group/#final-project-progress-reports","title":"Final Project Progress Reports","text":"<p>Starting around Week 6 your group will give a short, 5-minute presentation to your classmates at the end of each week. These progress reports should be no more than 3-5 slides discussing what goals you have met, what you are still working on, what is working, and what still needs debugging.</p>"},{"location":"course-deliverables/individual/","title":"Individual Assignments","text":""},{"location":"course-deliverables/individual/#donkeysim","title":"DonkeySim","text":"<p>During this course, you will need to upload a video of your trained model car successfully completing 3 autonomous laps on the DonkeySim track using three different scenarios that are outlined in the Virtual Machine Guide.</p>"},{"location":"course-deliverables/individual/#3-autonomous-laps-on-localhost","title":"3 Autonomous Laps on <code>localhost</code>","text":"<p>TODO: Link to DonkeyCar documentation for local host</p>"},{"location":"course-deliverables/individual/#training-on-the-gpu-cluster","title":"Training on the GPU Cluster","text":"<p>TODO: Link to documentation for GPU cluster</p>"},{"location":"course-deliverables/individual/#3-autonomous-laps-on-remote-server-roboticistdev","title":"3 Autonomous Laps on remote server <code>roboticist.dev</code>","text":"<p>TODO: Link to documentation for remote server</p>"},{"location":"course-deliverables/individual/#the-construct","title":"The Construct","text":"<p>TODO: link online course details</p>"},{"location":"course-overview/outline/","title":"Introductions and Class Logistics","text":""},{"location":"course-overview/outline/#instruction-team","title":"Instruction Team","text":"<ul> <li>Jack Silberman - jacks@eng.ucsd.edu - PhD, Faculty MAE, ECE and HDSI</li> <li>TBD - email - TA/Tutor</li> </ul>"},{"location":"course-overview/outline/#safety","title":"Safety","text":"<ul> <li>Follow the return to learn, that simple<ul> <li>Lab access. Please observe UCSD\u2019s safety guidelines - Compliance with required trainings.</li> <li>Be safe, ask for help; don\u2019t be afraid to test things but be safe. You are in school to learn, you are supposed to have help.</li> </ul> </li> </ul>"},{"location":"course-overview/outline/#adapting-to-changes","title":"Adapting to Changes","text":"<ul> <li>For the past 5+ years, more than 1/2 of you would not be in this class. Be patient with us.<ul> <li>We expanded it from 28 to 60 students, 7 to 15 Teams.</li> <li>This means we had to find a space to fit all of you in a lab environment. Please help by not talking at the same time as the instructional team.</li> </ul> </li> <li>We basically have double of the workload with the same resources.<ul> <li>Do your part and be patient when things don\u2019t work. Do some hacking and look for solutions with us. Don\u2019t expect solutions given to you without doing some work too.</li> </ul> </li> <li>We are using cutting-edge technology; it may cut us a bit while we master it.</li> </ul>"},{"location":"course-overview/outline/#logistics","title":"Logistics","text":"<ul> <li>Lectures and times are mixed since we do hands-on lab within the lectures.<ul> <li>Tuesdays and Thursdays: 5:30 - 6:50 PM<ul> <li>Office hours at the lab: 7:00 - 8:30 PM</li> </ul> </li> <li>Additional office hours TBD as needed</li> </ul> </li> <li>If needed, access to EBUII 339 TritonAI lab TBD with TA</li> <li>Discord for communication, Professor Silberman will share the invite link</li> <li>Assignments will be posted on Discord channels<ul> <li>Video evidence of course deliverables to be shared on Discord</li> </ul> </li> <li>See Grading Formula</li> <li>Top requests from past quarters<ul> <li>More time for final project</li> <li>Help on Python coding</li> <li>More structure on \"what\" and \"when\"</li> </ul> </li> <li>This is a 4 credit class. If you cannot dedicate 10 hours per week, please consider giving a chance to the dozens of other students on the waiting list. If you put in the effort, you will be successful.</li> <li>This class is hands-on-fun-busy where you will gain highly useful skills to add to your resume.</li> <li>There are no exams or lists of exercises to work on.<ul> <li>Your time will be used learning new skills that can make you stand out on job or graduate school applications.</li> <li>There are two 360 evaluations where your teammates will judge you. Also, other teams can help on your grade. More on that in grading formula.</li> </ul> </li> <li>CBT / e-book license on Robot Ignite Academy (The Construct) is required. You have homework already.</li> </ul>"},{"location":"course-overview/outline/#curriculum","title":"Curriculum","text":"<ul> <li>Deep Learning AI - Human Behave Cloning<ul> <li>Simulator on a virtual machine on your host computer.</li> <li>AI model Training using UCSD\u2019s Supercomputer Center.</li> <li>Multi-robots race online against racers from around the world.</li> <li>Autonomous laps using a Physical robot at UCSD\u2019s scale race track.</li> <li>Optional - We were invited for an event sponsored by a Bank in the Bay Area<ul> <li>Date to be confirmed - end of the quarter</li> <li>10 to 15 students sponsored to travel for a day in Oakland California</li> <li>Help companies founder train their robots with their kids then race; possible network opportunity for you.</li> </ul> </li> </ul> </li> </ul>"},{"location":"course-overview/syllabus/","title":"10-Week Course Syllabus","text":""},{"location":"course-overview/syllabus/#week-1","title":"Week 1","text":""},{"location":"course-overview/syllabus/#lecture-and-lab","title":"Lecture and Lab","text":"<ul> <li>Class expectations</li> <li>\"What will a robotics class enable me to do?\"<ul> <li>Search web for robotics jobs, i.e. keywords 'deep learning,' 'ROS2,' 'AI'</li> </ul> </li> <li>Grading formula</li> <li>Syllabus<ul> <li>Classes schedule and high level deliverables</li> </ul> </li> <li>Introduction to the class resources<ul> <li>Laboratory and tool</li> </ul> </li> <li>Laboratory and safety training</li> <li>Design robot, start building robot</li> <li>Deep Learning<ul> <li>Virtual Machines</li> <li>DonkeyCar: running on student host computer</li> <li>DonkeySim: running on student host computer</li> </ul> </li> <li>CBT/e-book<ul> <li>Embedded Linux</li> <li>Python</li> <li>Concepts of ROS2</li> </ul> </li> </ul>"},{"location":"course-overview/syllabus/#assignments-due","title":"Assignments Due","text":"<ul> <li>Begin designing robot<ul> <li>Electronics mount plate</li> <li>3D print camera mount</li> <li>3D print single board computer case</li> </ul> </li> <li>DonkeyCar DonkeySim<ul> <li>3 autonomous laps running on student host computer (due Tuesday of Week 2)</li> </ul> </li> <li>Linux and Python traning modules</li> <li>ROS2 Basics in 5 Days (Python) Section: Introduction</li> </ul>"},{"location":"course-overview/syllabus/#week-2","title":"Week 2","text":""},{"location":"course-overview/syllabus/#lecture-and-lab_1","title":"Lecture and Lab","text":"<ul> <li>Team members review and adjust. Let\u2019s lock the teams. Starting this week it will be very hard for people to join and catch.</li> <li>Deep Learning<ul> <li>Review of Virtual Machines and Host Machines<ul> <li>We use a virtual machine image from a hypervisor company called VMware</li> <li>The instructions to use our virtual machine is here</li> </ul> </li> <li>DonkeyCar - running on students host computer</li> <li>DonkeySim - running on the external server</li> </ul> </li> <li>Embedded Linux on a low power single board computer (SBC)<ul> <li>How can we install software without a computer monitor, keyboard, and mouse connected to an embedded computer (single board computer)?</li> </ul> </li> <li>Installing the software into the SBC</li> <li>Jetson Nano Single Board Computer (SBC) Hands-on<ul> <li>Remote access without a monitor, keyboard, mouse<ul> <li>Initial connection using a USB cable</li> <li>Connect to a local WiFi access point e.g., UCSDRoboCar</li> </ul> </li> <li>Multi-user on a low power SBC</li> <li>User Security</li> <li>Installing software using Secure Shell (SSH)</li> <li>Remote Desktop</li> </ul> </li> <li>Robot Components and Electronics</li> <li>GPS Based Navigation<ul> <li>GNSS (GPS) 3D Localization</li> <li>RTK GNSS Error Correction<ul> <li>Base Station</li> <li>Services<ul> <li>Open source</li> <li>Paid services</li> </ul> </li> </ul> </li> </ul> </li> <li>Robot design completed, and major components in place<ul> <li>Completed<ul> <li>Electronics mount plate</li> <li>3D printed camera mount</li> <li>3D printed case for the single board computer (SBC)</li> </ul> </li> <li>Incorporate<ul> <li>GNSS unit and antenna</li> <li>Electronics wiring</li> </ul> </li> </ul> </li> <li>CBT / e-book<ul> <li>ROS2 Basics: Topics, Launch files</li> </ul> </li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_1","title":"Assignments Due","text":"<ul> <li>Donkey Car DonkeySim 3 autonomous laps - sim running on the external server (due Thursday of Week 2)</li> <li>Basic software setup on Jetson (Jetpack, WiFi, hostname, DonkeyCar, etc.) (due Tuesday of Week 3)</li> <li>Robot components ready (Thursday of Week 3)<ul> <li>Mechanical Components</li> <li>Electrical Components</li> <li>Software - Linux, Jetpack, OpenCV GPU Accelerated, DonkeyCar </li> <li>Deep Learning laps</li> <li>DonkeyCar GNSS Navigation</li> </ul> </li> <li>ROS2 Basics in 5 Days (Python) Sections: Basic Concepts, Topics</li> </ul>"},{"location":"course-overview/syllabus/#week-3","title":"Week 3","text":""},{"location":"course-overview/syllabus/#lecture-and-lab_2","title":"Lecture and Lab","text":"<ul> <li>UCSD\u2019s SuperComputer GPU Cluster Deep Learning acceleration</li> <li>Hands-on GPS/GNSS Based Navigation - putting all together - In font of EBU I<ul> <li>3 outdoors autonomous laps using GNSS</li> </ul> </li> <li>3 Autonomous Laps Deep Learning on EBUII outdoor track</li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_2","title":"Assignments Due","text":"<ul> <li>3 Autonomous Laps GNSS / GPS - EBU I (Tuesday of Week 4)</li> <li>3 Autonomous Laps Deep Learning - EBU II (Thursday of Week 4)</li> <li>ROS2 Basics in 5 Days (Python) Section: Services</li> </ul>"},{"location":"course-overview/syllabus/#week-4","title":"Week 4","text":""},{"location":"course-overview/syllabus/#lecture","title":"Lecture","text":"<ul> <li>Introduction to Docker and Git</li> <li>Introduction to UCSD ROS 2 Robocar Framework </li> <li>Demo of Python Camera Based Navigation</li> <li>Introduction to Class Final Project requirements</li> <li>Introduction to Project Management</li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_3","title":"Assignments Due","text":"<ul> <li>Class Final Project Proposal (Tuesday Week 5)</li> <li>3 Autonomous Laps ROS2 (Thursday Week 5)</li> <li>ROS2 Basics in 5 Days (Python) Section: Actions</li> <li>ROS2 Guidebook</li> </ul>"},{"location":"course-overview/syllabus/#week-5","title":"Week 5","text":""},{"location":"course-overview/syllabus/#lecture_1","title":"Lecture","text":"<ul> <li>Neural Network</li> <li>Car Dynamics</li> <li>Final Project Status Update</li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_4","title":"Assignments Due","text":"<ul> <li>Weekly progress report on final project</li> </ul>"},{"location":"course-overview/syllabus/#weeks-6-10","title":"Weeks 6 - 10","text":""},{"location":"course-overview/syllabus/#lecture_2","title":"Lecture","text":"<ul> <li>As needed/per request<ul> <li>Suggested topics:<ul> <li>Filtering/state estimation</li> <li>Path planning: GPS waypoint, LiDAR, SLAM</li> <li>PID control</li> <li>ROSBAGS, rviz, rqt</li> </ul> </li> </ul> </li> </ul>"},{"location":"course-overview/syllabus/#assignments-due_5","title":"Assignments Due","text":"<ul> <li>Weekly presentation progress report on final project</li> <li>Bonus: ROS2 NAV Course</li> </ul>"},{"location":"guidebooks/100-ucsd-robocar/","title":"UCSD Robocar Framework","text":"<p>The UCSD Robocar framework is primarily maintained and developed by Dominic Nightingale right here at UC San Diego.</p> <p>UCSD Robocar uses ROS and ROS2 for controlling our scaled robot cars which can vary from traditional programming or machine learning to achieve an objective. The framework works with a vast selection of sensors and actuation methods in our inventory making it a robust framework to use across various platforms. Has been tested on 1/16, 1/10, 1/5 scaled robot cars and soon our go-karts.</p>"},{"location":"guidebooks/100-ucsd-robocar/#about","title":"About","text":"<p>This framework was originally developed as one of Dominic\u2019s senior capstone projects as an undergraduate and has been under constant development throughout his graduate program. The framework provides the ability to easily control a car-like robot as well as performing autonomous tasks. It is currently being used to support his thesis in learning-model predictive control (LMPC).</p> <p>The framework is also being used to teach undergraduates the fundamentals of using gitlab, docker, python, openCV and ROS. The students are given the task to use the framework with their robots to perform autonomous laps on a track by first going through a calibration process that's embedded into the framework. The students then have to come up with their own final projects for the class that can be supported by the framework, which can vary from car following, SLAM applications, path planning, city driving behaviors, Human-machine-interfacing and so much more.</p>"},{"location":"guidebooks/100-ucsd-robocar/#whats-being-used","title":"What's Being Used","text":""},{"location":"guidebooks/100-ucsd-robocar/#embedded-computers","title":"Embedded Computers","text":"<p>There are 3 main computers that have been used to develop and test this framework which belong to the NVIDIA Jetson family. </p> <ul> <li>Jetson Nano</li> <li>Jetson Xavier Nx</li> <li>Jetson AGX Xavier</li> </ul>"},{"location":"guidebooks/100-ucsd-robocar/#ubuntu","title":"Ubuntu","text":"<p>The host OS on all the Jetson computers use Ubuntu18 which is flashed through NVIDIA's Jetpack image. However, the docker image uses Ubuntu20 in order to use ROS2 without worrying about package installation issues.</p>"},{"location":"guidebooks/100-ucsd-robocar/#gitlab","title":"GitLab","text":"<p>This is where all the code for the entire framework is managed and developed. Gitlab provides a service similar to google drive but for programs! It's especially convenient in terms of deploying code into embedded computers.</p>"},{"location":"guidebooks/100-ucsd-robocar/#docker","title":"Docker","text":"<p>This tool is being used to expedite the setup process on the computers. To get the docker image working, the Jetson just needs to be flashed with the Jetpack 4.6 image provided by NVIDIA and then simply pull the UCSD Robocar docker image from docker hub onto the Jetson. This allows for plug-n-play capabilities as long as all the hardware is connected to the Jetson properly.</p>"},{"location":"guidebooks/100-ucsd-robocar/#rosros2","title":"ROS/ROS2","text":"<p>The framework allows for both ROS-Noetic and ROS2-Foxy to work together through the ROS bridge or independently depending on the application.</p>"},{"location":"guidebooks/100-ucsd-robocar/#recommendations","title":"Recommendations","text":""},{"location":"guidebooks/100-ucsd-robocar/#vs-code-ide","title":"VS Code IDE","text":"<p>Microsoft Visual Studio IDE is an excellent development tool for coding especially because of all the free plug-ins that can be added.</p> <p>Plug-ins recommended:</p> <ul> <li>Python</li> <li>Docker</li> <li>Remote SSH</li> </ul>"},{"location":"guidebooks/100-ucsd-robocar/#virtual-machines","title":"Virtual Machines","text":"<p>If having software related issues, a virtual machine can possibly solve the issues and also provide a linux based interface to use with the jetson which is usually much smoother than with Windows or Mac.</p> <p>For details on how to install Virtual Machine software, see the Virtual Machine Guide section of this website.</p> <p>To download a virtual machine image that runs Ubuntu20.04, has VS code (with all plug-ins mentioned above), Docker and the UCSDrobocar docker image installed already, click here.</p> <p>Once you've downloaded the image and set it up with VMware, use the following credentials for login.</p> <p>Hostname: <code>ucsdrobocar-vm</code> Username: <code>robocar</code> Password: <code>ucsdrobocar</code> </p>"},{"location":"guidebooks/100-ucsd-robocar/#ucsd-robocar-framework-breakdown","title":"UCSD Robocar Framework Breakdown","text":"<p>Below are the supporting packages to the framework. The Nav package operates as the \"brain\" because it is the only package that communicates to all the other packages which are all independent from one another. \u200b Why so many packages? In practice, developing stand-alone or independent functionalities makes the package more robust in terms of deployability. Also as the robot becomes more sophisticated, the number of packages it will have access to would naturally increase allowing it to achieve many different types of tasks depending on the application of interest.</p> <p>So the idea is to develop a package that could in general be used on any car-like robot as well as being able to choose what packages your robot really needs without having to use the entire framework.  \u200b For example, lets say another company developed their own similar sensor, actuator and nav packages but they have not researched into lane detection. Instead of using the entire UCSD Robocar framework, they could easily just deploy the lane detection package and have some interpreter in their framework read the messages from the lane detection package to suit their needs.</p> <p>Link to official GitLab repo (ROS): ucsd_robocar_hub1 Link to official GitLab repo (ROS2): ucsd_robocar_hub2</p> <p>Note</p> <p>Both <code>hub1</code> and <code>hub2</code> are metapackages. For specific details about any individual package, click on any of the packages in either hub to be taken to that packages' main repository.</p>"},{"location":"guidebooks/100-ucsd-robocar/#packages","title":"Packages","text":"<p>Each UCSD ROS package has a README.md that explains in detail what config, nodes, launch files it has as well as topic/message information. So if you are confused about a particular thing, ask yourself: \u201cWhat is the problem I am having?\u201d and \u201cWhat package is most likely the root of the concern?\u201d Then go see the readme for that package and check anything relevant or even the troubleshooting section.</p> <p>In the package sections below are the links to the official README.md docs for each package for both ROS1 and ROS2. So any package with a 1 in it is for ROS-NOETIC and any package with a 2 is for ROS2-FOXY.</p>"},{"location":"guidebooks/100-ucsd-robocar/#nav","title":"Nav","text":"<p>The navigation package (nav_pkg) is the \"brain\" of the UCSD Robocar framework because it keeps all the launch files in its package to launch any node/launch file from the other packages used in the framework. This makes using the framework easier because you only really have to remember the name of the nav_pkg and what launch file you want to use rather than having to remember all the other package names and their own unique launch files.</p> <p>Resources: <code>NAV2_README.md</code> <code>NAV1_README.md</code></p>"},{"location":"guidebooks/100-ucsd-robocar/#lane-detection","title":"Lane Detection","text":"<p>The lane detection package is one method of navigating by identifying and tracking road markers. The basic principle behind this package is to detect road markers using openCV and then compute whats called the \u201ccross-track-error\u201d which is the difference between the center axis of the car and the centroid (center of \u201cmass\u201d) of the road mark which is then fed into a PID controller for tracking.</p> <p>Resources: <code>Lane_Detection2_README.md</code> <code>Lane_Detection1_README.md</code></p>"},{"location":"guidebooks/100-ucsd-robocar/#sensor","title":"Sensor","text":"<p>The sensor package contains all the required nodes/launch files needed to use the sensors that are equipped to the car. <code>Sensor2 README.md</code> <code>Sensor1 README.md</code></p>"},{"location":"guidebooks/100-ucsd-robocar/#actuator","title":"Actuator","text":"<p>The actuator package contains all the required nodes/launch files needed to use the actuators that are equipped to the car. <code>Actuator2 README.md</code> <code>Actuator1 README.md</code></p>"},{"location":"guidebooks/100-ucsd-robocar/#control-coming-soon","title":"Control (coming soon)","text":"<p>The control package contains all the required nodes/launch files needed to control the car in various methods such as PID, LQR, LQG and MPC.</p>"},{"location":"guidebooks/100-ucsd-robocar/#path-coming-soon","title":"Path (coming soon)","text":"<p>The path package contains all the required nodes/launch files needed to create trajectories for the car to follow in a pre-built map as well as in simulations </p>"},{"location":"guidebooks/100-ucsd-robocar/#basics","title":"Basics","text":"<p>The path package contains all the required nodes/launch files needed to subscribe/publish to the sensor/actuator messages within the framework for fast algorithm prototyping.</p> <p><code>Basics2 README.md</code></p>"},{"location":"guidebooks/100-ucsd-robocar/#updating-all-packages","title":"Updating all packages","text":"<p>A utility function was added to the <code>~/.bashrc</code> script that will automatically update all the packages in the framework and then rebuild and source it so it will be ready to start using ROS2!</p> <p>From a terminal, run:</p> <pre><code>upd_ucsd_robocar\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#launch-files","title":"Launch Files","text":"<p>The launch file diagrams below show the very general approach of how the packages communicate with one another. With ROS, it just comes down to a combination of starting launch files and sending messages (through topics) to nodes. For specific details about messages types, topics, services and launch files used, please go to the readme for the specific package of interest!</p> <p>The nav_pkg is at the base of each of the diagrams and rooting from it are the launch files it calls that will launch other nodes/launch files from all the other packages in the framework.</p> <p>In ROS2, a dynamically built launch file (at run-time) is used to launch all the different nodes/launch files for various purposes such as data collection, navigation algorithms and controllers. </p> <p>This new way of creating launch files has now been simplified by just adding an entry to a <code>yaml</code> file of where the launch file is and a separate <code>yaml</code> file to indicate to use that launch file or not. </p> <p>There is only one file to modify, and all that needs to be changed is either putting a <code>0</code> or a <code>1</code> next to the list of nodes/launch files. To select the nodes that you want to use, put a <code>1</code> next to it otherwise put a <code>0</code> which means it will not activate. </p> <p>In the figures below, instead of including the entire <code>ros2 launch</code> command, you will only see the names of the launch files that need to be turned on in the node config file explained more in detail here.</p> <p>TODO: images of node mapping</p>"},{"location":"guidebooks/100-ucsd-robocar/#developer-tools","title":"Developer Tools","text":""},{"location":"guidebooks/100-ucsd-robocar/#ros-guidebooks","title":"ROS Guidebooks","text":"<p>Links provided below are guides for ROS and ROS2 which include many examples, terminal commands and general concept explanations of the various features in ROS and ROS2.</p> <ul> <li>UCSD ROS Guidebook</li> <li>UCSD ROS2 Guidebook</li> </ul>"},{"location":"guidebooks/100-ucsd-robocar/#gitlab_1","title":"GitLab","text":"<p>Since the framework uses a meta package (a package that contains multiple packages) we refer to individual packages as submodules.</p>"},{"location":"guidebooks/100-ucsd-robocar/#adding-new-submodules","title":"Adding new submodules","text":"<pre><code>git submodule add &lt;remote_url&gt;\ngit commit -m \"message\"\ngit push\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#updating-local-submodules-with-remote-submodules","title":"Updating local submodules with remote submodules","text":"<p>If local changes have been made, the update command will fail unless you add, commit and push (shown in the following subsection) or stash (<code>git stash</code>) them, which will temporarily discard any local changes.</p> <pre><code>git submodule update --remote --merge\n</code></pre> <p>Pay attention to the output of this command to make sure it did not fail or abort.</p>"},{"location":"guidebooks/100-ucsd-robocar/#updating-remote-submodules-with-local-submodules","title":"Updating remote submodules with local submodules","text":"<pre><code>git add .\ngit commit -m \"message\"\ngit push\n</code></pre> <p>Again, pay attention to the output.</p>"},{"location":"guidebooks/100-ucsd-robocar/#removing-submodules","title":"Removing submodules","text":"<pre><code>git submodule deinit &lt;submodule&gt;\ngit rm &lt;submodule&gt;\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#adding-an-existing-package-to-git","title":"Adding an existing package to git","text":"<p>From the web browser, create an empty repo on GitLab.</p> <p>Now from the Jetson, start by creating a new ROS2 package.</p> <pre><code>ros2 pkg create --build-type ament_python pkg_name --dependencies rclpy\nbuild_ros2\n</code></pre> <p>Now proceed with merging the new package into the framework.</p> <pre><code>git init\ngit remote add origin &lt;remote-url-from-step-one&gt;\ngit add .\ngit commit -m \"message\"\ngit push --set-upstream origin master\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#docker-commands","title":"Docker Commands","text":"<p>Below is a go-to list of docker commands that can be used with the framework.</p> <p>Some new lingo: Container name = <code>NAME</code> Image name = <code>REPOSITORY</code> Image tag ID (comparable to branches in git) = <code>TAG</code></p>"},{"location":"guidebooks/100-ucsd-robocar/#pullingrunning","title":"Pulling/running","text":"<p>To pull an image from Docker Hub:</p> <pre><code>docker pull REPOSITORY:TAG\n</code></pre> <p>Starting a stopped container:</p> <pre><code>docker start NAME\n</code></pre> <p>Stopping a running container:</p> <pre><code>docker stop NAME\n</code></pre> <p>To open a new terminal for a Docker container that is currently running:</p> <pre><code>docker exec -it NAME bash\n</code></pre> <p>Building Docker image by giving it a new name and tag:</p> <pre><code>docker build -t REPOSITORY:TAG .\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#updatingcreatingsharing","title":"Updating/creating/sharing","text":"<p>To save changes made while in container to the original image, change the tag to create a new image including your changes:</p> <pre><code>docker commit NAME REPOSITORY:TAG\n</code></pre> <p>Creating a new image from a container:</p> <pre><code>docker tag NAME REPOSITORY:TAG\n</code></pre> <p>Pushing image to Docker Hub:</p> <pre><code>docker push REPOSITORY:TAG\n</code></pre> <p>To share files between host and docker container:</p> <ul> <li>From host to container:</li> </ul> <pre><code>docker cp &lt;filename&gt;.txt &lt;container-ID&gt;:/&lt;filename&gt;.txt\n</code></pre> <ul> <li>From container to host:</li> </ul> <pre><code>docker cp &lt;container-ID&gt;:/&lt;filename&gt;.txt &lt;filename&gt;.txt\n</code></pre> <p>Note</p> <p>Both of these commands need to be run from a host terminal, they won't work if you're inside the Docker container.</p>"},{"location":"guidebooks/100-ucsd-robocar/#listing","title":"Listing","text":"<p>To list all images:</p> <pre><code>docker images\n</code></pre> <p>To list all running containers:</p> <pre><code>docker ps\n</code></pre> <p>To list all containers including those not running:</p> <pre><code>docker ps -a\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#deleting","title":"Deleting","text":"<p>To delete a specific container:</p> <pre><code>docker rm NAME\n</code></pre> <p>To delete a specific image:</p> <pre><code>docker rmi REPOSITORY:TAG\n</code></pre> <p>To delete ALL containers:</p> <pre><code>docker rm -f $(docker ps -a -q)\n</code></pre> <p>To delete ALL images:</p> <pre><code>docker rmi -f $(docker images -q)\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#accessing-docker-images","title":"Accessing Docker images","text":"<p>Currently there are two DIFFERENT docker images that are being supported by UCSD. One image was built for arm architecture computers (Jetson family) and the other was built for X86 architecture computers (most laptops and desktops). Apple M1 support will be coming soon.</p> <p>Question: Why two images?</p> <p>Answer: The X86 image was built to provide an environment for the developer to test new algorithms, packages, sensors (Yes, you can plug sensors into your computer just like the Jetson for testing) etc in a simulated environment without having to use a physical robot. Using the physical robot for first-time testing can lead to damaging the robot or something/someone in the environment due to an unforeseen behavior from the robot. We must practice safe autonomy if we ever hope to see our new ideas become a part of the industry! This leads to the ARM image, which was built to be used on the physical robot when ready to perform physical testing.</p> <p>Question: The display wont open when in the container, how to make it work? (ie. images won't port through)</p> <p>Answer: There could be several reasons why the display is not working but below are the most common solutions that can be tried:</p> <ul> <li>Make sure that an X11 forwarding session was established when running SSH connection into the JTN.</li> <li>If that still does not work, the container could have a broken connection with the display. If this is the case, try creating a new container using the provided function in the <code>~/.bashrc</code> script.</li> </ul> <p>Note</p> <p>Docker is pre-installed on the Jetson computers so no need to install it, but in order to use the X86 image, you must install Docker on your computer.</p>"},{"location":"guidebooks/100-ucsd-robocar/#ucsd-robocar-image","title":"UCSD Robocar Image","text":"<p>The image is hosted on Docker Hub here.</p> <ul> <li>Computer architechture <code>ARM</code> (Jetson) To pull the image from the terminal:</li> </ul> <pre><code>docker pull djnighti/ucsd_robocar:latest\n</code></pre> <ul> <li>Computer architecture <code>X86</code> (PC) To pull the image from the terminal:</li> </ul> <pre><code>docker pull djnighti/ucsd_robocar:x86\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#docker-setup","title":"Docker Setup","text":"<p>The exact \"recipe\" to build this image can be found here.</p> <p>Note</p> <p>If you are using the virtual machine, all of this is already completed for you!</p>"},{"location":"guidebooks/100-ucsd-robocar/#enable-x11-port-forwarding","title":"Enable X11 Port Forwarding","text":"<p>From your host machine (not the Jetson) enter these commands. You will have to enter this in your terminal every time before you can SSH into the JTN.</p> <pre><code>xhost +\nssh -X jetson@your-ip-address\n</code></pre> <p>Now, from inside the JTN, run the following commands to obtain <code>sudo</code> privileges for docker commands. This only needs to be run one time.</p> <pre><code>sudo usermod -aG docker $(USER)\nsu $(USER)\n</code></pre> <p>Now, to see if everything is working correctly, test your X11 forwarding with:</p> <pre><code>xeyes\n</code></pre> <p>If you see some googly eyes pop up on your screen, X11 is ready to go. </p> <p>If X11 port forwarding is not set up on your machine, follow the steps here to get it started. Then return to these steps and try them again.</p>"},{"location":"guidebooks/100-ucsd-robocar/#update-docker-daemon","title":"Update Docker daemon","text":"<p>To ensure Docker containers can be run properly on the JTN, modify the <code>daemon.json</code> file by deleting the previous version and creating a new one.</p> <pre><code>sudo rm /etc/docker/daemon.json\nsudo nano /etc/docker/daemon.json\n</code></pre> <p>Now copy and paste the following into the file you just created.</p> <pre><code>{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"default-runtime\": \"nvidia\"\n}\n</code></pre> <p>Save, quit, and reboot the JTN for your changes to be applied.</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"guidebooks/100-ucsd-robocar/#running-a-container-on-the-jtn","title":"Running a container on the JTN","text":"<p>SSH back into the JTN after you reboot, with the <code>-X</code> flag indicating X11 forwarding is enabled.</p> <pre><code>ssh -X jetson@your-ip-address\n</code></pre> <p>We can now create a shortcut to easily run a container with all the necessary arguments from the command line by adding a function to the <code>~/.bashrc</code> file.</p> <pre><code>gedit ~/.bashrc\n</code></pre> <p>At the very bottom of the file, paste the following.</p> <pre><code>robocar_docker ()\n{\n    docker run \\\n    --name ${1}\\\n    -it \\\n    --privileged \\\n    --net=host \\\n    -e DISPLAY=$DISPLAY \\\n    -v /dev/bus/usb:/dev/bus/usb \\\n    --device-cgroup-rule='c 189:* rmw' \\\n    --device /dev/video0 \\\n    --volume=\"$HOME/.Xauthority:/root/.Xauthority:rw\" \\\n    djnighti/ucsd_robocar:${2:-devel}\n}\n</code></pre> <p>Notice the 2 arguments we have made:</p> <ul> <li><code>${1}</code>: This will be the name of the container, i.e. <code>test_container</code></li> <li><code>${2:devel}</code>: This is the tag ID of the image you want to launch a container from. If nothing is specified when calling at the command line (example shown below), the <code>devel</code> tag will be run automatically.</li> </ul> <p>Don't modify the function. The arguments are intentional and not meant to be hard-coded. <p>Once you've added the lines to your <code>~/.bashrc</code> and saved the file, source the script so your current terminal can see the new function you just added. Then, run the command to enter a new container.</p> <pre><code>source ~/.bashrc\nrobocar_docker test_container\n</code></pre> <p>This will start a container called <code>test_container</code> built from the image with default tag ID <code>devel</code>.</p> <p>To access this same container from another terminal:</p> <pre><code>docker exec -it test_container bash\n</code></pre> <p>At this point, your Docker setup is complete, but don't forget to refer to the useful Docker commands which include deleting, creating and updating images locally and remotely!</p>"},{"location":"guidebooks/30-robocar-jetson/","title":"UCSD Robocar Jetson Nano Configuration","text":""},{"location":"guidebooks/30-robocar-jetson/#introduction","title":"Introduction","text":"<p>The NVIDIA Jetson Nano in this document is also referred as JTN.</p> <p>To learn more about the JTN, you can check out the NVIDIA documentation sources Getting Started, Jetson Download Center, and the documentation for usage on DonkeyCar.</p> <p>!!! info \"     In general for this course if you are using a computer with a Linux distribution like Ubuntu you will have an easier time installing the necessary Artificial Intelligence framework. Linux uses a file format that is not easily read in an Apple MacOS or MS Windows computer.  If you need to modify files in the \\(\\mu\\)SD card formatted for Linux, e.g., a disk partition that uses Ext4 format, you should ask a colleague first if they have a Linux PC, then Tutors or TA for help.</p> <p>Every Team is expected to try installing the necessary software based on these instructions using the \\(\\mu\\)SD image, not the recovery image.  And every member of the team needs to participate during the lab embedded linux hands-on exercise. The same SBC (JTN) can be used at the class by several users without the need to connect a monitor and a keyboard to it. It is part of your learning in the course to get familiar with Embedded Linux, Head-Less Single Board Computers (SBC) like the JTN, terminal running in a PC, SSH, and remotely installing software in a SBC.</p> <p>If you run out of time, based on instructor\u2019s set deliverables, or your \\(\\mu\\)SD card gets corrupted, you can use the recovery \\(\\mu\\)SD card image that has all the software pre-installed. On the 10-UCSD Robocar ECE &amp; MAE 148 document there is a link to a recovery \\(\\mu\\)SD card image.</p>"},{"location":"guidebooks/30-robocar-jetson/#flashing-microsd-card-for-the-jetson-nano","title":"Flashing microSD card for the Jetson Nano","text":"<p>From your PC let's prepare (flash) the microSD card (\\(\\mu\\)SD card). Make sure your computer can access the Internet. If you are using one of our WiFi Access Points in one of the labs, or at one of the tracks, the first device that connects to the WiFi Access point and try to access the Internet, will need to accept the UCSD Wireless Visitor Agreement, just like when you are connecting to UCSD\u2019s Visitor WiFi. In fact you are basically connecting to the Internet via the UCSD\u2019s Visitor WiFi. </p>"},{"location":"guidebooks/30-robocar-jetson/#software-installation","title":"Software Installation","text":"<p>Etcher can use Zipped files, you don\u2019t need to Unzip the image file if you are using Etcher. Ignore if your Windows or Mac computer tells you it can not ready the \\(\\mu\\)SD card. The \\(\\mu\\)SD card will be used on the JTN running Linux. It uses a file format that Windows and Mac don\u2019t know about unless you use specialized software to do that. You don\u2019t need it. If you are using Linux or MacOS command lines to write the disk image to a \\(\\mu\\)SD card, you may need to extract the file first.</p> <p>Download the UCSD Jetson Nano Developer Kit \\(\\mu\\)SD Card Image here.</p> <p>Once you have the image, install Etcher to be able to write the disk image to a microSD card here.</p> <p>Using the image we are providing as a starting point has some advantages with some pre-configuration. For example, it won't require a monitor and a keyboard connected to the JTN to get you started.</p>"},{"location":"guidebooks/30-robocar-jetson/#writing-an-image-to-a-musd-card","title":"Writing an image to a \\(\\mu\\)SD card","text":"<p>Connect a \\(\\mu\\)SD adapter to your PC.</p> <ol> <li>Insert the provided \\(\\mu\\)SD card (64 gigabytes) into the \\(\\mu\\)SD adapter.</li> <li>Install and run Etcher.</li> <li>Start Etcher, choose the Zipped file with the Disk Image you downloaded, pay attention when choosing the drive with the \\(\\mu\\)SD card on it (e.g., 64 gigabytes).</li> <li>Write the image to \\(\\mu\\)SD card.</li> <li>Eject the \\(\\mu\\)SD card from your PC first using the procedure for your PC Operating System (e.g., eject drive) </li> <li>Then insert the \\(\\mu\\)SD card into the JTN \\(\\mu\\)SD card slot. Note this is a push-in-to-lock and push-in-to-unlock the \\(\\mu\\)SD card. Please do not pull the \\(\\mu\\)SD card out of the slot before unlocking it, otherwise you may damage your JTN and or \\(\\mu\\)SD card.</li> </ol>"},{"location":"guidebooks/30-robocar-jetson/#powering-on-the-jtn","title":"Powering on the JTN","text":"<p>The preferable way to power the JTN is with the provided 5V 4A power supply; it has a barrel connector that plugs into the JTN. You need to place a jumper connector at J48, there is a text label close to it that says ADD JUMPER TO DISABLE USB PWR. </p> <p>For the initial configuration you don\u2019t have to use the 5V 4A power supply. If you are not using the provided power supply for the initial configuration, you need to remove the jumper located above the JNT power connectors to allow it to be powered by the uUSB cable. Please save the jumper. You can leave it in place connected to one of the pins of J48.</p> <p>Power on the JTN by connecting the power supply to a power outlet and the barrel jack into the barrel port on the Jetson. </p> <p>Note</p> <p>Give the Jetson a minute or two to boot and load its software and the fan may not work until you install the software later in these instructions.</p>"},{"location":"guidebooks/30-robocar-jetson/#wired-communication-with-the-jtn","title":"Wired communication with the JTN","text":"<p>We will use the command line to access the Jetson Nano (JTN) using a secure shell (SSH). Don\u2019t worry, if you are not familiar with terminals and using command lines, you will master it in this class. Mastering these will be a good skill to have and another mention in your resume.</p> <p>The Linux OS running on the JTN enables two protocols of communication: Serial Communication and SSH. Both use a local network interface with TCP/IP over the USB cable. By using a micro USB cable between the JTN and the PC, you can communicate with the JTN. On the PC, you need to use software to enable serial communication or SSH. e.g., Serial Communication - screen on Linux or MacOS. CoolTerm is another option that runs on Linux, MacOS, and MS Windows. SSH is natively supported on Linux and MacOS. MS Windows 10 or later supports SSH too.</p> <p>Note</p> <p>Your JTN should have a WiFi / Bluetooth network interface and antennas already installed on it. If not, please contact the Tutors or TAs.</p>"},{"location":"guidebooks/30-robocar-jetson/#ssh-communication-via-usb-connection","title":"SSH Communication (via USB connection)","text":"<p>Connect a \\(\\mu\\)USB cable between the JTN and your PC. Then, from a terminal, run:</p> <pre><code>ssh jetson@192.168.55.1\n</code></pre> <p>Or:</p> <pre><code>ssh jetson@ucsdrobocar-xxx-yy.local\n</code></pre> <p>The default UserID and password is:</p> <p>Username: <code>jetson</code>  Password: <code>jetsonucsd</code></p>"},{"location":"guidebooks/30-robocar-jetson/#serial-communication","title":"Serial Communication","text":"<p>If needed, install a software called <code>screen</code> on your host or virtual Linux computer and/or update it.</p> <pre><code>sudo apt-get install screen\nsudo apt-get update screen\n</code></pre> <p>Or you can install coolterm, or use any terminal emulator software you have. On my computer the JTN plugged in using the uUSB cable shows up as <code>/dev/ttyACM1</code>. Log-in info when doing a serial connection is the same as the SSH connection</p> <p>The command line to connect in a Linux terminal is:</p> <pre><code>screen /dev/ttyACM1\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#wireless-communication-with-the-jtn","title":"Wireless communication with the JTN","text":"<p>After connecting to the JTN via USB connection, let's configure the JTN to connect to a WiFi Access Point. Using SSH via USB connection or using the serial communication software log into the JTN.</p> <p>First, let's make sure the network service is running.</p> <pre><code>sudo systemctl start networking.service\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#accessing-wifi-networks","title":"Accessing WiFi networks","text":"<p>To list the available WiFi networks:</p> <pre><code>sudo nmcli device wifi list\n</code></pre> <p>If the WiFi Access point that you want to connect is not initially listed try rescan to refresh the list. If after several tries the network still does not show, try rebooting by typing <code>sudo reboot</code>. After your machine has restarted:</p> <pre><code>sudo nmcli device wifi rescan\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#connecting-to-a-wifi-network","title":"Connecting to a WiFi network","text":"<p>We have bridged WiFi access points. Try to stay in the 5GHz network. The command to connect he JTN to an access point is</p> <pre><code>sudo nmcli device wifi connect &lt;ssid_name&gt; password &lt;password&gt;\n</code></pre> <p>Here are the WiFi access points we use:</p> <pre><code>ssid=\"UCSDRoboCar5GHz\"\npassword=\"UCSDrobocars2018\"\nssid=\"UCSDRoboCar\"\npassword=\"UCSDrobocars2018\"\nssid=\"SD-DIYRoboCar5GHz\" \npassword=\"SDrobocars2017\"\nssid=\"SD-DIYRoboCar\"\npassword=\"SDrobocars2017\"\n</code></pre> <p>So, to connect, an example command could look like:</p> 5GHz<pre><code>sudo nmcli device wifi connect UCSDRoboCar5GHz password UCSDrobocars2018\n</code></pre> 2.4GHz<pre><code>sudo nmcli device wifi connect UCSDRoboCar password UCSDrobocars2018\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#get-ip-address-of-jtn","title":"Get IP address of JTN","text":"<p>After connecting the JTN to a WiFi Access Point, let's find out the IP address the JTN is getting from the network so we can connect to it remotely. </p> <p>In a terminal:</p> <pre><code>ifconfig\n</code></pre> <p>You should see some output that contains the following line:</p> <pre><code>inet 192.168.222.167\n</code></pre> <p>Note</p> <p>Your IP address may be slightly different. It's important to use the corresponding <code>inet</code> IP address output onto your screen when configuring your JTN.</p> <p>In the future, to connect to a new Access Point you may have to repeat these steps using a \\(\\mu\\)USB cable connected between your PC and the JTN.</p> <p>Helpful Hint: You can add your phone as an Access Point so you always have a backup connection to your JTN. If you have your phone with you, you can connect to the JTN and then add another WiFi using SSH.</p>"},{"location":"guidebooks/30-robocar-jetson/#ssh-communication-via-wifi","title":"SSH communication (via WiFi)","text":"<p>Connect your PC to the JTN using WiFi (using example IP address from above) then enter the username and password.</p> <pre><code>ssh jetson@192.168.222.167\nUsername: jetson\nPassword: jetsonucsd\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#update-wifi-power-settings","title":"Update WiFi power settings","text":"<p>Lets turn off the power saving for the WiFi device.</p> <p>If your SSH connection is slow or lagging, make sure you have the power saving on the WiFi disabled.</p> <pre><code>sudo iw dev wlan0 set power_save off\n</code></pre> <p>Now let's install a small text editor called <code>nano</code>.</p> <pre><code>sudo apt-get update\nsudo apt-get install nano\n</code></pre> <p>Lets make the change on the WiFi power saving settings persistent. We need to edit a file. </p> <pre><code>sudo nano /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf\n</code></pre> <p>In the file, change</p> <pre><code>wifi.powersave = 3\n</code></pre> <p>to </p> <pre><code>wifi.powersave = 2\n</code></pre>"},{"location":"guidebooks/30-robocar-jetson/#system-settings","title":"System Settings","text":"<p>Now lets change the name of the JTN (host name) and password of the JTN. We will configure the name of the JTN based on your Class <code>xxx</code> and Team <code>yy</code>. Hostname <code>ucsdrobocar-xxx-yy</code> where <code>xxx-yy</code> is your class and team number, ex: <code>148-05</code>, <code>190-01</code>, <code>190-TA1</code>, <code>190-TA2</code> Example: <code>ucsdrobocar-148-05</code> for Team 5</p> <p>To see your current computer information:</p> <pre><code>hostnamectl\n</code></pre> <p>insert screenshot</p>"},{"location":"guidebooks/30-robocar-jetson/#changing-your-hostname","title":"Changing your hostname","text":"<p>Replace <code>xxx-yy</code> as shown above with your class name and team number.</p> <pre><code>sudo hostnamectl set-hostname ucsdrobocar-xxx-yy\n</code></pre> <p>Then there is one more place to change the hostname:</p> <pre><code>sudo nano /etc/hosts\n</code></pre> <p>insert screenshot</p> <p>Change the name <code>jetson</code> here or any other name it may have for your JTN, for example</p> <pre><code>127.0.0.1   localhost\n127.0.1.1   ucsdrobocar-xxx-yy\n</code></pre> <p>When making edits using <code>nano</code>, to save the file, first press Ctrl+O to write out your changes. Press Enter to save them under the same filename, and then press Ctrl+X to exit.</p>"},{"location":"guidebooks/50-vm-guide/","title":"Virtual Machine with DonkeyCar/DonkeySim AI Simulator","text":""},{"location":"guidebooks/50-vm-guide/#initial-setup","title":"Initial Setup","text":""},{"location":"guidebooks/50-vm-guide/#download-and-install","title":"Download and Install","text":"<p>Download VMware Player (Windows and Linux), Fusion Player (MacOS). Not working for Mac M1 or M2 yet.</p> <p>A VMware virtual machine image can be downloaded from here. Download and unzip it onto your host computer.</p> <p>Once unzipped the virtual machine image will take about 40 GB of disc space. If needed, clear some space from your computer, i.e. removing videos or other large media files. Alternatively, use a USB external drive if needed.</p> <ul> <li>If you unzip the image onto an external USB, you will have to ensure the USB is connected each time you run the virtual machine on VMware.  </li> <li>If VMware Player asks you \"Did you copy or move these files?\" select \"I copied them.\"</li> </ul> <p>You need a host machine with at least 8 GB RAM. The virtual machine is configured by default to use 5 GB RAM. If your host machine has 16 GB or more, you will need to configure your settings in VMware to use 8 GB. You can change this setting in your virtual hardware configuration while the virtual machine is not running.</p>"},{"location":"guidebooks/50-vm-guide/#initial-boot-of-vm","title":"Initial Boot of VM","text":"<p>Start the virtual machine using the VMware player (Windows, Linux) or Fusion Player (MacOS). - If needed to boot, enable Virtualization on your BIOS/EFI - On the VMware Player machine configuration, disable \"Optimization for Virtualization.\" - Example of settings for Fusion Player on Mac:</p>"},{"location":"guidebooks/50-vm-guide/#log-into-vmware-virtual-machine","title":"Log into VMware virtual machine","text":"<p>Username: <code>ucsd</code>  Password: <code>UcsdStudent</code></p>"},{"location":"guidebooks/50-vm-guide/#copypaste-capabilitites","title":"Copy/Paste Capabilitites","text":"<p>If cutting and pasting from your host computer to your virtual machine is not working, open a terminal in the Linux VM and run the following:</p> <pre><code>sudo apt-get autoremove open-vm-tools\nsudo apt-get install open-vm-tools-desktop\nsudo reboot\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#connecting-game-controller","title":"Connecting Game Controller","text":"<p>Using a game controller will be the desirable way to control your DonkeyCar simulation. Examples of compatible game controllers include PS3, PS4, Xbox, Logitech F710.</p> <p>The controller needs to be connected to the host computer using USB cable, Bluetooth or USB dongle.</p> <p>Once the controller is connected to the host, the VMware Player should ask you where you want to connect the game controller ('Connect to the host' or 'Connect to a virtual machine'). Select the UCSD-Ubuntu machine.</p> <p>The pop-up window should appear as one of the following:</p> <p>image here</p> <p>If you don't see this option, reconnect the controller.</p> <p>To verify that the virtual machine can see the game controller, check the input for <code>js0</code>.</p> <ul> <li>Open a terminal in Linux machine and run the following command.</li> </ul> <pre><code>ls /dev/input\n</code></pre> <p>You should see:</p> <pre><code>by-id    event0  event2  event4  event6  js0   mouse0  mouse2\nby-path  event1  event3  event5  event7  mice  mouse1  mouse3\n</code></pre> <p>Once you confirm that you can see <code>js0</code>, let's test the joystick controls.</p> <ul> <li>From a terminal:</li> </ul> <pre><code>sudo apt-get update\nsudo apt-get install -y jstest-gtk\njstest /dev/input/js0\n</code></pre> <p>image of joystick controls</p> <p>Move/press the game controller buttons to test which buttons trigger the corresponding input values, i.e. X, O, Triangle, Select/Share, Start/Options.</p> <p>Jack, what if my game controller works with the <code>jtest /dev/input/js0</code> but is not working with the DonkeyCar?</p> <p>Not to worry, there is a way to build a custom file for your game controller with the controls we use most.</p>"},{"location":"guidebooks/50-vm-guide/#custom-controller-if-neededdesired","title":"Custom controller (if needed/desired)","text":"<p>If you have a controller that is not listed above, or you are having trouble getting your controller to work, or you simply want to map your controller differently, see the Custom Game Controller documentation on DonkeyCar.</p> <p>To discover or modify the button and axis mappings for your controller, you can use the Joystick Wizard via the command line. </p> <p>The Joystick Wizard will write a custom controller named <code>my_joystick.py</code> to your <code>mycar</code> folder. To use the custom controller, set <code>CONTROLLER_TYPE=\"custom\"</code> in your <code>myconfig.py</code>.</p> <ul> <li>From a terminal, navigate to your <code>mycar</code> directory.</li> </ul> <pre><code>cd ~/projects/d4_sim\n</code></pre> <p>This should be the directory with all of your Python configuration files, i.e. <code>manage.py</code>, <code>train.py</code>, <code>myconfig.py</code>.</p> <ul> <li>First, make sure the OS can access your device. The utility <code>jtest</code> can be usefull here. If needed:</li> </ul> <pre><code>sudo apt install joystick\n</code></pre> <p>You must pass this utility the path to your controller's device. Typically this will be <code>/dev/input/js0</code>. However, if it is not, you must find the correct device path and provide it to the utility. You will need this for the <code>createjs</code> command as well.</p> <ul> <li>From the current directory you have just navigated to, run the command</li> </ul> <pre><code>donkey createjs\n</code></pre> <p>This will create a file named <code>my_joystick.py</code> in your <code>/d4_sim</code> folder, next to your <code>manage.py</code>.</p> <p>Modify <code>myconfig.py</code> to use your new controller.</p> <pre><code>atom myconfig.py\n</code></pre> <p>Find the line below and modify it as:</p> <pre><code>CONTROLLER_TYPE=\"custom\"\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#donkeycar-ai-framework","title":"DonkeyCar AI Framework","text":"<p>DonkeyCar AI Framework Explained</p>"},{"location":"guidebooks/50-vm-guide/#how-to-launch-the-simulator","title":"How to launch the simulator","text":"<p>To start the DonkeySim:</p> <ul> <li>Use the File Explorer to navigate to <code>~/projects/DonkeySimLinux</code>.  </li> <li>Double click the file <code>donkey_sim.x86_64</code> to execute.  </li> <li>You should now see the DonkeySim ready for use.</li> </ul> <p>Depending on the track we will be racing on, you need to train on the track that will be used during the race in order to successfully race against other people.</p> <p>Some tracks you will see are <code>donkey-circuit-launch-track-v0</code>, <code>donkey-warren-track-v0</code>, <code>donkey-mountain-track-v0</code>.</p> <p>Specify track for current quarter</p>"},{"location":"guidebooks/50-vm-guide/#customize-your-virtual-robot-racer","title":"Customize your virtual robot racer","text":"<ul> <li>From a terminal:</li> </ul> <pre><code>cd ~/projects/d4_sim\natom myconfig.py\n</code></pre> <p>This time you will pull up the <code>myconfig.py</code> file to investigate and edit more in-depth.</p> <p>In your <code>myconfig.py</code> you may modify the following variables</p> <pre><code>GYM_CONF[\"racer_name\"] = \"UCSD-148-YourName\"\nGYM_CONF[\"country\"] = \"USA\"\nGYM_CONF[\"bio\"] = \"Something_about_you, ex: Made in Brazil\"\n</code></pre> <p>in addition to the <code>car_name</code> variable in the following line.</p> <pre><code>GYM_CONF = { \"body_style\" : \"car01\", \"body_rgb\" : (255, 205, 0), \"car_name\" : \"UCSD-148-YourName\", \"font_size\" : 30}\n</code></pre> <p>You can also change the color of the car to one of UCSD's colors, blue or gold. You can see how by examining the commented lines in the demo file below.</p> <p>An example <code>myconfig.py</code> file:</p> myconfig.py<pre><code># 04Jan22\n# UCSD mods to make easier for the UCSD students to use the Donkey-Sim\n# the following uncommented lines where copied here from the body of myconfig.py below\nDONKEY_GYM = True\n# DONKEY_SIM_PATH = \"remote\"\nDONKEY_SIM_PATH = \"/home/ucsd/projects/DonkeySimLinux/donkey_sim.x86_64\"\n# DONKEY_GYM_ENV_NAME = \"donkey-warren-track-v0\"\nDONKEY_GYM_ENV_NAME = \u201cdonkey-mountain-track-v0\u201d\n# UCSD yellow color in RGB = 255, 205, 0\n# UCSD blue color in RGB = 0, 106, 150\nGYM_CONF = { \"body_style\" : \"car01\", \"body_rgb\" : (255, 205, 0), \"car_name\" : \"UCSD-148-YourName\", \"font_size\" : 30} # body style(donkey|bare|car01) body rgb 0-255\nGYM_CONF[\"racer_name\"] = \"UCSD-148-YourName\"\nGYM_CONF[\"country\"] = \"USA\"\nGYM_CONF[\"bio\"] = \"Something_about_you, ex: Made in Brazil\"\n#\n# SIM_HOST = \"donkey-sim.roboticist.dev\"\nSIM_ARTIFICIAL_LATENCY = 0\nSIM_HOST = \"127.0.0.1\"  \n# when racing on virtual-race-league use host \"roboticist.dev\"\n# SIM_ARTIFICIAL_LATENCY = 30\n# when training on the host machine, \n# set artificial latency to the value \n# when you ping roboticist.dev. When racing on \n# virtual-race league, use 0 (zero)\n#\n# When racing, to give the ai a boost, configure these values.\nAI_LAUNCH_DURATION = 3     # the ai will output throttle for this many seconds\nAI_LAUNCH_THROTTLE = 1     # the ai will output this throttle value\nAI_LAUNCH_KEEP_ENABLED = True     # when False (default) you will need to hit the \n# AI_LAUNCH_ENABLE_BUTTON for each use. \n# This is safest. When this True, is active on each trip into \"local\" ai mode.\n#\n# JOYSTICK\n# When using a joystick modify these by uncommenting USE_JOYSTICK_AS_DEFAULT = True\n#\n# USE_JOYSTICK_AS_DEFAULT = True  \n# when starting the manage.py, when True, will not require a --js option to use the joystick\nJOYSTICK_MAX_THROTTLE = 1.0      # this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don't need the full speed available.\nJOYSTICK_STEERING_SCALE = 0.8    # some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\nAUTO_RECORD_ON_THROTTLE = True   # if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\nJOYSTICK_DEADZONE = 0.2      # when non zero, this is the smallest throttle before recording triggered.\n# #Scale the output of the throttle of the ai pilot for all model types.\nAI_THROTTLE_MULT = 1.0     # this multiplier will scale every throttle value for all output from NN models\n#\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#get-latency-from-remote-server-roboticistdev","title":"Get latency from remote server <code>roboticist.dev</code>","text":"<p>First, ping the remote server and take note of the average ping time, i.e. 30 ms.</p> <pre><code>ping donkey-sim.roboticist.dev\n</code></pre> <p>insert image of ping results</p> <p>In this example, since I am in the same network as <code>donkey-sim.roboticist.dev</code> my ping time is much smaller, around 0.5 ms. Assuming you are somewhere in the USA, you should get between 20-60 ms.</p> <p>Write this value into your <code>myconfig.py</code> file as shown above.</p> <pre><code>SIM_ARTIFICIAL_LATENCY = 30\n</code></pre> <p>Remember: This step is crucial to train your car for remote racing when you are not on the same network in the future. </p>"},{"location":"guidebooks/50-vm-guide/#collecting-data","title":"Collecting data","text":"<p>For training your model, we will do a \"behavioral cloning\" AI.</p> <p>Begin by driving the robot in the local simulator, aka the simulator on your virtual machine that is running on your host.</p> <p>If you have not already, activate the donkey virtual environment.</p> <pre><code>conda activate donkey\n</code></pre> <p>If you have activated the venv correctly, your terminal line should now look something like this:</p> <pre><code>(donkey) ucsd@ucsd-virt-ub:~/$\n</code></pre> <p>Navigate to your DonkeyCar directory.</p> <pre><code>cd ~/projects/d4_sim\n</code></pre> <p>Now let's drive the robot to collect data automatically. In your terminal, run:</p> <pre><code>python manage.py drive\n</code></pre> <p>You should see the following output.</p> <pre><code>You can now go to http://localhost:8887 to drive your car.\nStarting vehicle at 20 Hz\n</code></pre> <p>Open a web browser like Chrome (or Firefox on Linux).</p> <pre><code>http://localhost:8887\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#driving-your-robot","title":"Driving your robot","text":"<p>To control your car, you can either use the controller after configuring it following the steps above, or you can drive with your keyboard and mouse from the web browser.</p> <p>Some helpful settings for your joystick to improve controllability are outlined in the <code># JOYSTICK</code> section of <code>myconfig.py</code>.</p> <pre><code># JOYSTICK\nUSE_JOYSTICK_AS_DEFAULT = True   # when starting the manage.py\nJOYSTICK_MAX_THROTTLE = 1.0      # this scalar is multiplied with the -1 to 1\nJOYSTICK_STEERING_SCALE = 0.7    # some people want a steering that is less sensitive\nAUTO_RECORD_ON_THROTTLE = True   # if true, will record whenever throttle is not zero\nCONTROLLER_TYPE='F710'           # (ps3|ps4|xbox|nimbus|wiiu|F710|rc3|MM1|custom)\nJOYSTICK_DEADZONE = 0.0          # when non zero, this is the smallest throttle before recording triggered\n</code></pre> <p>Hint: if your trained model is having trouble with accuracy when going around curves, try setting <code>AUTO_RECORD_ON_THROTTLE = False</code> and engage your throttle manually. This way there will not be any gaps in recording data.</p> <p>Some advantages to using a game controller over your mouse/keyboard:</p> <ul> <li>Delete 100 last data points (at 20 Hz, this is equivalent to the last 5 seconds of driving data)<ul> <li>On PS3/PS4 this is usually \"Triangle\" button</li> <li>On Logitech F710 this is usually \"Y\"</li> <li>If you miss a turn or accidentally drive your vehicle into a wall, just remove that 5 seconds of recording and keep going</li> </ul> </li> <li>Emergency Stop</li> <li>Easily switch operation modes from AI to manual </li> </ul> <p>Practice for a while and don't worry about not driving well initially. We'll show you how to delete all the data you've collected in one run so you can start with a clean set later.</p> <p>We will be training the AI model in increments of approximately 20 laps, so count your laps as feasible.</p> <p>To stop the DonkeyCar, use <code>Ctrl+C</code> like other Python code you've used so far.</p>"},{"location":"guidebooks/50-vm-guide/#deleting-data","title":"Deleting data","text":"<p>You can delete your entire data directory to start fresh if you'd like. This can be done with basic Linux commands you've recently learned.</p> <p>Where is the data stored? Take a guess.</p> <pre><code>cd ~/projects/d4_sim\nls\n</code></pre> <p>Inside your directory you should see a folder called <code>/data</code>. Let's clean it up and start fresh.</p> <pre><code>rm -rf data\n</code></pre> <p>Then, create the directory again.</p> <pre><code>mkdir data\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#training-and-testing","title":"Training and testing","text":"<p>OK, so you've recorded 20 laps of driving data. Now what?</p> <p>It's time to train your model using all the data by giving it a name when running the <code>train.py</code> command. From your <code>/d4_sim</code> directory, run the command:</p> <pre><code>python train.py --model=models/14mar24_sim_160x120_20.h5 --type=linear --tub=./data\n</code></pre> <p>By executing this command with the <code>--model</code> option, you are telling the framework to train a new model called <code>14mar24_sim_160x120_20.h5</code> with the data you've collected, with the <code>20</code> tag at the end indicating how many laps you're using to train with.</p> <p>Depending on your connection speed and the amount of laps you've recorded, this could take a while. Once your model has been trained, let's test it.</p> <pre><code>python manage.py drive --model=models/14mar24_sim_160x120_20.h5 --type=linear\n</code></pre> <p>This should open up the simulator as before. Press \"Select\" twice on your controller to start the AI model (or whichever button corresponds with your controller to activate AI drive).</p>"},{"location":"guidebooks/50-vm-guide/#does-your-model-work","title":"Does your model work?","text":"<p>A lot of things may have just happened.</p> <ul> <li>Does your car go berserk? Don't worry, there's a few things you can try.  <ul> <li>Add another 20 laps and train again</li> <li>Go into your <code>myconfig.py</code> and modify the sections of the code that configure the driving behavior of your AI model.  <ul> <li>This could consist of increasing or decreasing the AI throttle boost, changing the max throttle, changing the throttle multiplier, etc.</li> </ul> </li> </ul> </li> <li>Does your car drive well but lose it around the corners?  <ul> <li>Check to make sure your <code>AUTO_RECORD_ON_THROTTLE</code> is set to <code>False</code>. This way you won't lose data when you release the gas around tight corners, and you can also manually choose when you want to record your driving and when to stop.</li> </ul> </li> </ul> <p>While driving your robot to collect data, it is a good idea to keep the terminal visible so you can see whether you are recording data. Sometimes you may accidentally turn recording off while you are driving and you may not know it.</p> <p>As you continue adding data by driving more laps manually, you can rename the model as you train. For example, if you add 20 more laps and train again, you may choose to change the <code>20</code> tag to <code>40</code> on your next train.</p> <pre><code>python train.py --model=models/14mar24_sim_160x120_40.h5 --type=linear --tub=./data\n</code></pre> <p>Remember to run this from your <code>/d4_sim</code> directory for it to work</p> <p>Now let's test your model again with the new data.</p> <pre><code>python manage.py drive --model=models/14mar24_160x120_40.h5 --type=linear\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#training-on-a-specific-tub","title":"Training on a specific tub","text":"<p>You may also choose to drive your model and store data in a different tub each time you add new laps.</p> <pre><code>cd ~/projects/d4_sim\nmkdir -p data/tub_2\npython manage.py drive --model=models/14mar24_160x120_40.h5 --type=linear --tub=data/tub_2\n</code></pre> <p>This instance creates a new directory in the <code>/data</code> directory, then initiates your simulator and points to the new tub for your driving data to be recorded.</p> <p>Add 20 more laps. Then, to train with your new data and transfer the training to your previous model, run the following command.</p> <pre><code>python train.py --tub=data/tub_1 --transfer=models/previous_model.h5 --model=models/new_model.h5\n</code></pre> <p>For this example, the command would be</p> <pre><code>python train.py --tub=data/tub_1 --transfer=models/14mar24_sim_160x120_40.h5 --model=models/14mar24_160x120_40.h5\n</code></pre>"},{"location":"guidebooks/50-vm-guide/#ucsd-gpu-cluster","title":"UCSD GPU Cluster","text":"<p>Please do not use the GPU cluster until you demonstrate training on your local machine first. It is part of your deliverables for the class.</p>"}]}